{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iovOr6U17X3T"
   },
   "source": [
    "# Setup path directory\n",
    "\n",
    "1.   import library\n",
    "2.   Setup Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJJpgvsvZ0LT"
   },
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 7777,
     "status": "ok",
     "timestamp": 1622982127748,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "v6Fq59KP728N"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "import random\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import pickle\n",
    "import math\n",
    "import cv2\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import keras\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.objectives import categorical_crossentropy\n",
    "\n",
    "from skimage import io \n",
    "\n",
    "from keras.models import Model\n",
    "from keras.utils import generic_utils\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Fw3QsYNaKzC"
   },
   "source": [
    "## Setup Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1622982127749,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "wwmi7OAJUIyo"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "\n",
    "  def __init__(self):\n",
    "    # Print the process or not\n",
    "    self.verbose = True\n",
    "    # Name of base network\n",
    "    self.network = 'vgg'\n",
    "    # Setting for data augmentation\n",
    "    self.use_horizontal_flips = False\n",
    "    self.use_vertical_flips = False\n",
    "    self.rot_90 = False\n",
    "\n",
    "    # Anchor box scales\n",
    "    # Note that if im_size is smaller, anchor_box_scales should be scaled\n",
    "    # Original anchor_box_scales in the paper is [128, 256, 512]\n",
    "    self.anchor_box_scales = [64, 128, 256] \n",
    "    # Anchor box ratios\n",
    "    self.anchor_box_ratios = [[1, 1], [1./math.sqrt(2), 2./math.sqrt(2)], [2./math.sqrt(2), 1./math.sqrt(2)]]\n",
    "\n",
    "    # Size to resize the smallest side of the image\n",
    "    # Original setting in paper is 600. Set to 300 in here to save training time\n",
    "    self.im_size = 300\n",
    "\n",
    "    # image channel-wise mean to subtract\n",
    "    self.img_channel_mean = [103.939, 116.779, 123.68]\n",
    "    self.img_scaling_factor = 1.0\n",
    "\n",
    "    # number of ROIs at once\n",
    "    self.num_rois = 4\n",
    "\n",
    "    # stride at the RPN (this depends on the network configuration)\n",
    "    self.rpn_stride = 16\n",
    "\n",
    "    self.balanced_classes = False\n",
    "\n",
    "    # scaling the stdev\n",
    "    self.std_scaling = 4.0\n",
    "    self.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]\n",
    "\n",
    "    # overlaps for RPN\n",
    "    self.rpn_min_overlap = 0.3\n",
    "    self.rpn_max_overlap = 0.7\n",
    "\n",
    "    # overlaps for classifier ROIs\n",
    "    self.classifier_min_overlap = 0.1\n",
    "    self.classifier_max_overlap = 0.5\n",
    "\n",
    "    # placeholder for the class mapping, automatically generated by the parser\n",
    "    self.class_mapping = None\n",
    "    self.model_path = None\n",
    "    self.base_weights_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1622982127749,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "XGSu5287aj7O"
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "config.use_horizontal_flips = True\n",
    "config.use_vertical_flips = True\n",
    "config.rot_90 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRA3_86Tfcqt"
   },
   "source": [
    "## Import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1622982127749,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "5GbVAYC-Cga5"
   },
   "outputs": [],
   "source": [
    "base_path = ''\n",
    "data_path = os.path.join(base_path,'data')\n",
    "train_path = os.path.join(data_path,'preprocessing/train.csv')\n",
    "output_weight_path = os.path.join(base_path,'model/model_faster_rcnn.h5')\n",
    "base_weights_path   = os.path.join(base_path,'model/vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "record_path = os.path.join(base_path,'model/record.csv')\n",
    "config_output_filename = os.path.join(base_path, 'config/model_vgg_config.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxB3GgfRuz1x"
   },
   "source": [
    "Save path to config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1622982127750,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "xHjuuKOeml7d"
   },
   "outputs": [],
   "source": [
    "config.record_path = record_path\n",
    "config.model_path  = output_weight_path\n",
    "config.base_net_weights = base_weights_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjA7EH3D86j-"
   },
   "source": [
    "# Read data from path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1622982127750,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "oxZg_d5fDA4Z"
   },
   "outputs": [],
   "source": [
    "def get_data(csv_path):\n",
    "  \"\"\"\n",
    "  parse the data from annotation file\n",
    "  Input:\n",
    "    csv_path: image csv file\n",
    "  Output:\n",
    "    all_data: \"filepath, width , height, list(bboxes)\n",
    "    Class_count\n",
    "    Class_mapping\n",
    "  \"\"\"\n",
    "  data = pd.read_csv(csv_path)\n",
    "  all_imgs = {}\n",
    "  classes_count = {}\n",
    "  class_mapping = {}\n",
    "  found_bg = False\n",
    "  path = 'data/train'\n",
    "  for i, row in data.iterrows():\n",
    "    # sys.stdout.write(str(i) + ', image: ' +str(row['FileName']) + '\\r')\n",
    "    # sys.stdout.flush()\n",
    "    img_fname = row['FileName']\n",
    "    img_path = os.path.join(path, img_fname)\n",
    "    img = io.imread(img_path)\n",
    "    height, width = img.shape[:2]\n",
    "    ## Note that the coordinate of cv2\n",
    "    #  0, 0 --------------------------->\n",
    "    #   |   x1, y1 --------------\n",
    "    #   |       |\n",
    "    #   |       |\n",
    "    #   v       -----------------x2, y2\n",
    "    class_name = row['ClassName']\n",
    "    x1 = int (row['XMin'] * width)\n",
    "    x2 = int (row['XMax'] * width)\n",
    "    y1 = int (row['YMin'] * height)\n",
    "    y2 = int (row['YMax'] * height)\n",
    "    if class_name not in classes_count:\n",
    "      classes_count[class_name] = 1\n",
    "    else:\n",
    "      classes_count[class_name]  += 1\n",
    "    if class_name not in class_mapping:\n",
    "      if class_name == 'bg' and found_bg == False:\n",
    "        print('We have background region in data')\n",
    "        found_bg = True\n",
    "      class_mapping[class_name] = len(class_mapping)\n",
    "    if img_fname not in all_imgs:\n",
    "      all_imgs[img_fname] = {}\n",
    "      all_imgs[img_fname]['filepath'] = img_path\n",
    "      all_imgs[img_fname]['width'] = width\n",
    "      all_imgs[img_fname]['height'] = height\n",
    "      all_imgs[img_fname]['bboxes'] = []\n",
    "    all_imgs[img_fname]['bboxes'].append({'class': class_name,\n",
    "                                        'x1': x1,\n",
    "                                        'x2': x2,\n",
    "                                        'y1': y1,\n",
    "                                        'y2': y2})\n",
    "\n",
    "  all_data = []\n",
    "  for key in all_imgs:\n",
    "      all_data.append(all_imgs[key])    \n",
    "  return all_data, classes_count,class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 325346,
     "status": "ok",
     "timestamp": 1622982453092,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "kEAxMgCiU2yg"
   },
   "outputs": [],
   "source": [
    "all_data, classes_count,class_mapping = get_data(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1622982453097,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "MxsjacLjt7vM",
    "outputId": "73f58de6-9e0e-49e2-c568-97d18ca4dc80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images per class:\n",
      "{'Car': 964, 'Mobile phone': 559, 'Person': 2215, 'bg': 0}\n",
      "Num classes (including bg) = 4\n",
      "{'Person': 0, 'Car': 1, 'Mobile phone': 2, 'bg': 3}\n"
     ]
    }
   ],
   "source": [
    "if 'bg' not in classes_count:\n",
    "\tclasses_count['bg'] = 0\n",
    "\tclass_mapping['bg'] = len(class_mapping)\n",
    "# e.g.\n",
    "#    classes_count: {'Car': 2383, 'Mobile phone': 1108, 'Person': 3745, 'bg': 0}\n",
    "#    class_mapping: {'Person': 0, 'Car': 1, 'Mobile phone': 2, 'bg': 3}\n",
    "config.class_mapping = class_mapping\n",
    "\n",
    "print('Training images per class:')\n",
    "pprint.pprint(classes_count)\n",
    "print('Num classes (including bg) = {}'.format(len(classes_count)))\n",
    "print(class_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYrnebYQvH9G"
   },
   "source": [
    "Save config to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1622982453097,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "Wxpc8_q-u9a0",
    "outputId": "62dccd5d-6e91-440b-db12-3e106282c08f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config has been written to config/model_vgg_config.pickle, and can be loaded when testing to ensure correct results\n"
     ]
    }
   ],
   "source": [
    "with open(config_output_filename, 'wb') as config_f:\n",
    "  pickle.dump(config,config_f)\n",
    "  print('Config has been written to {}, and can be loaded when testing to ensure correct results'.format(config_output_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1622982453098,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "4QYlDUD1t-kV",
    "outputId": "8ddafde4-1d77-4b7d-e26f-9c2c033b3275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train samples (images) 1138\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the images with seed\n",
    "random.seed(1)\n",
    "random.shuffle(all_data)\n",
    "\n",
    "print('Num train samples (images) {}'.format(len(all_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_D561pYU-up"
   },
   "source": [
    "# Creat anchor for all images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqV60UHukGnr"
   },
   "source": [
    "## Calculate IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1622982453098,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "mOVR8PoGkJ-N"
   },
   "outputs": [],
   "source": [
    "def intersection(bbox, abox):\n",
    "  '''\n",
    "  Calculate intersection of anchor box and bounding box\n",
    "  Input:\n",
    "    bbox is array which contains coordinates of bounding box\n",
    "    abox is array which contains coordinates of anchor box\n",
    "  Output:\n",
    "    intersection of anchor box and bounding box\n",
    "  '''\n",
    "  x1 = max(bbox[0], abox[0])\n",
    "  x2 = min(bbox[1], abox[1])\n",
    "  y1 = max(bbox[2], abox[2])\n",
    "  y2 = min(bbox[3], abox[3])\n",
    "  inters = (x2 - x1) * (y2 - y1)\n",
    "  if (x2 - x1) < 0 or (y2 - y1) < 0:\n",
    "    return 0.0\n",
    "  else:\n",
    "    return inters \n",
    "\n",
    "def union(bbox, abox, inters):\n",
    "  '''\n",
    "  Calculate union of anchor box and bounding box\n",
    "  Input:\n",
    "    bbox is array which contains coordinates of bounding box\n",
    "    abox is array which contains coordinates of anchor box\n",
    "    Inter is intersection of anchor box and bounding box\n",
    "  Output:\n",
    "    union of anchor box and bounding box\n",
    "  '''\n",
    "  a_area = (abox[1] - abox[0]) * (abox[3] - abox[2]) # anchor box area\n",
    "  b_area = (bbox[1] - bbox[0]) * (bbox[3] - bbox[2]) # bounding box area\n",
    "  uni = a_area + b_area - inters\n",
    "  return uni\n",
    "\n",
    "def iou(bbox, abox):\n",
    "  if (bbox[1] < bbox[0]) or (bbox[3] < bbox[2]) or (abox[1] < abox[0]) or (abox[3] < abox[2]):\n",
    "    return 2.0\n",
    "  area_i = intersection(bbox, abox)\n",
    "  area_u = union(bbox, abox, area_i)\n",
    "  assert area_u != 0\n",
    "  return float(area_i)/float(area_u)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhtA3o-Jzi3m"
   },
   "source": [
    "## Calculate the rpn for all anchors of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1622982453100,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "CYkKkjIWV1qn"
   },
   "outputs": [],
   "source": [
    "def get_img_output_length(width, height):\n",
    "  '''\n",
    "  Calculate size of feature map after neurons network beacause it vgg so we device it by 16 (based on the network architecture)\n",
    "  '''\n",
    "  return int(width / 16), int(height / 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 499,
     "status": "ok",
     "timestamp": 1622982453588,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "etA7eZbRQUg6"
   },
   "outputs": [],
   "source": [
    "def creat_anchorbox(config, img_data):\n",
    "  downscale = float(config.rpn_stride)\n",
    "  anchor_sizes = config.anchor_box_scales\n",
    "  anchor_ratios = config.anchor_box_ratios\n",
    "  n_asizes = len(anchor_sizes) # number of anchor sizes\n",
    "  n_aratios = len(anchor_ratios) # number of anchor ratios\n",
    "  n_anchors = n_asizes * n_aratios # number of anchors\n",
    "  num_bboxes = len(img_data['bboxes'])\n",
    "  width = img_data['width']\n",
    "  height = img_data['height']\n",
    "\n",
    "  '''\n",
    "  num_anchors_for_bbox is the array which contains the number of anchors for each bounding box\n",
    "  best_anchor_for_bbox is the array which contains the coordinates of the best anchor box for each bounding box\n",
    "  best_iou_for_bbox is the array which contains the iou value for the best anchor box for each bounding box\n",
    "  best_x_for_bbox is array which contains the coordinates of anchor box for each bounding box\n",
    "  best_dx_for_bbox is array which contains the parameterization of 4 coordinates for each bounding box\n",
    "  '''\n",
    "  num_anchors_for_bbox = np.zeros(num_bboxes).astype(int) #[[number of i-th bbox ]]\n",
    "  best_anchor_for_bbox = -1 *np.ones((num_bboxes, 4)).astype(int) # [[mapping to the featurn map], [size, ratio]]\n",
    "  best_iou_for_bbox = np.zeros(num_bboxes).astype(np.float32) #[[best_iou_for_this_bbox]]\n",
    "  best_x_for_bbox = np.zeros((num_bboxes, 4)).astype(int) # [[x1, x2, y1, y2]]\n",
    "  best_dx_for_bbox = np.zeros((num_bboxes,4)).astype(np.float32) # [[tx, ty, tw, th]]\n",
    "  \n",
    "  bboxes = np.zeros((num_bboxes, 4))\n",
    "  for i, bbox in enumerate (img_data['bboxes']):\n",
    "    bboxes[i, 0] = bbox['x1']\n",
    "    bboxes[i, 1] = bbox['x2']\n",
    "    bboxes[i, 2] = bbox['y1']\n",
    "    bboxes[i, 3] = bbox['y2']\n",
    "\n",
    "  # calculate the map size base on the network architecture\n",
    "  output_width, output_height = get_img_output_length(width, height)\n",
    "\n",
    "  # initialize empty output objective\n",
    "  '''\n",
    "  y_rpn_overlap show ability the box is object\n",
    "  y_is_box_valid show ability the box is valid or not\n",
    "  y_rpn_rerg show coordinate of each each anchor\n",
    "  '''\n",
    "  y_rpn_overlap = np.zeros((output_height, output_width,n_anchors)) # sth like (18, 25, 9) if width and height of feature map is (25, 18)\n",
    "  y_is_box_valid = np.zeros((output_height, output_width, n_anchors)) # sth like (18, 25, 9) if width and height of feature map is (25, 18)\n",
    "  y_rpn_regr = np.zeros((output_height, output_width, (n_anchors * 4))) # sth like (18,25, 36) if condition like above\n",
    "  iou_list = np.zeros((output_height, output_width,n_anchors)).astype(np.float16)\n",
    "  \n",
    "  # rpn ground truth\n",
    "  for size_idx in range (n_asizes):\n",
    "    for ratio_idx in range (n_aratios):\n",
    "      # Calculate width and height of anchor box\n",
    "      width_anchor =  anchor_sizes[size_idx] * anchor_ratios[ratio_idx][0] \n",
    "      height_anchor = anchor_sizes[size_idx] * anchor_ratios[ratio_idx][1]\n",
    "      # Consider each point of feature map (output) is the mapping of each anchor box\n",
    "      for ix in range (output_width):\n",
    "        # x-coordinate of anchor box\n",
    "        x1_anc = downscale * (ix + 0.5) - width_anchor / 2\n",
    "        x2_anc = downscale * (ix + 0.5) + width_anchor / 2\n",
    "        # check if x-coordinate are outside the width\n",
    "        if (x1_anc < 0) or (x2_anc > width):\n",
    "          continue \n",
    "        for jy in range (output_height):\n",
    "          # y-coordinate of anchor box\n",
    "          y1_anc = downscale * (jy + 0.5) - height_anchor / 2\n",
    "          y2_anc = downscale * (jy + 0.5) + height_anchor / 2\n",
    "          # check if y-coordinate are outside the height \n",
    "          if (y1_anc < 0) or (y2_anc > height):\n",
    "            continue\n",
    "          # initialize anchor box with status 'negative'\n",
    "          bbox_type = 'neg'\n",
    "          # creat a variable which save the best IOU for each center position of anchor box\n",
    "          best_iou_for_loc = 0.0\n",
    "          for bbox_idx in range (num_bboxes):\n",
    "            curr_iou = iou(bboxes[bbox_idx], [x1_anc, x2_anc, y1_anc, y2_anc])\n",
    "            #plot each anchor to each bounding box\n",
    "            iou_list[jy, ix, ratio_idx + n_aratios * size_idx] = curr_iou\n",
    "            if curr_iou > best_iou_for_bbox[bbox_idx] or curr_iou > config.rpn_max_overlap:\n",
    "              # compute center of bbox\n",
    "              cx = (bboxes[bbox_idx, 1] + bboxes[bbox_idx, 0]) / 2.0\n",
    "              cy = (bboxes[bbox_idx, 2] + bboxes[bbox_idx, 3]) / 2.0\n",
    "              # compute center of bbox\n",
    "              cxa = (x1_anc + x2_anc) / 2.0\n",
    "              cya = (y1_anc + y2_anc) / 2.0\n",
    "              # compute the parameterizations of 4 coordinate\n",
    "              '''\n",
    "              tx = (x - xa) / wa # wa - width of anchor box\n",
    "              ty = (y - ya) / ha # ha - heigth of anchor box\n",
    "              tw = log(w / wa)   # w  - width of bounding box\n",
    "              th = log(h / ha)   # h  - height of bounding box\n",
    "              '''\n",
    "              tx = (cx - cxa) / width_anchor\n",
    "              ty = (cy - cya) / height_anchor\n",
    "              tw = np.log((bboxes[bbox_idx, 1] - bboxes[bbox_idx, 0]) / width_anchor)\n",
    "              th = np.log((bboxes[bbox_idx, 3] - bboxes[bbox_idx, 2]) / height_anchor)\n",
    "            if img_data['bboxes'][bbox_idx]['class'] != 'bg':\n",
    "              # every ground truth bounding box need to map to an anchor box ----> we need to choose the best anchor box \n",
    "              if curr_iou > best_iou_for_bbox[bbox_idx]:\n",
    "                best_anchor_for_bbox[bbox_idx, :] = [jy, ix, ratio_idx, size_idx]\n",
    "                best_iou_for_bbox[bbox_idx] = curr_iou\n",
    "                best_x_for_bbox[bbox_idx,:] = [x1_anc, x2_anc, y1_anc, y2_anc]\n",
    "                best_dx_for_bbox[bbox_idx,:]= [tx, ty, tw, th]\n",
    "              # if the IOU is equal than 0.7, we set anchor status is positive\n",
    "              if curr_iou > config.rpn_max_overlap:\n",
    "                bbox_type = 'pos'\n",
    "                num_anchors_for_bbox[bbox_idx] += 1\n",
    "                # update the regression layer target if this IOU is the best for the current (x, y) coordinate and anchor position\n",
    "                if curr_iou > best_iou_for_loc:\n",
    "                  best_iou_for_loc = curr_iou\n",
    "                  best_regr = (tx, ty, tw, th)\n",
    "              if config.rpn_min_overlap < curr_iou < config.rpn_max_overlap:\n",
    "                # gray zone between neg and pos\n",
    "                if bbox_type != 'pos':\n",
    "                  bbox_type = 'neu'\n",
    "            # Set output of y_is_box_valid and y_rpn_overlap :\n",
    "          if bbox_type == 'neg':\n",
    "            y_is_box_valid[jy, ix, ratio_idx + n_aratios * size_idx] = 1\n",
    "            y_rpn_overlap[jy, ix, ratio_idx + n_aratios * size_idx] = 0\n",
    "          elif bbox_type == 'neu':\n",
    "            y_is_box_valid[jy, ix, ratio_idx + n_aratios * size_idx] = 0\n",
    "            y_rpn_overlap[jy, ix, ratio_idx + n_aratios * size_idx] = 0\n",
    "          elif bbox_type == 'pos':\n",
    "            y_is_box_valid[jy, ix, ratio_idx + n_aratios * size_idx] = 1\n",
    "            y_rpn_overlap[jy, ix, ratio_idx + n_aratios * size_idx] = 1\n",
    "            start = 4 * ( ratio_idx + n_aratios * size_idx) \n",
    "            y_rpn_regr[jy, ix, start:start + 4] = best_regr\n",
    "  # we need ensure that every bbox has at least one positive RPN region\n",
    "  for idx in range (num_bboxes):\n",
    "    if num_anchors_for_bbox[idx] == 0:\n",
    "      # no box with an IOU greater than zero  \n",
    "      if best_anchor_for_bbox[idx, 0] == -1:\n",
    "        continue\n",
    "      jy = best_anchor_for_bbox[idx, 0]\n",
    "      ix = best_anchor_for_bbox[idx, 1]  \n",
    "      ratio_idx = best_anchor_for_bbox[idx,2]\n",
    "      size_idx = best_anchor_for_bbox[idx,3]\n",
    "      #------------------------------------      \n",
    "      y_is_box_valid[jy, ix, ratio_idx + n_aratios * size_idx] = 1\n",
    "      y_rpn_overlap[jy, ix, ratio_idx + n_aratios * size_idx]  = 1\n",
    "      start = 4 * ( ratio_idx + n_aratios * size_idx) \n",
    "      y_rpn_regr[jy, ix, start:start + 4] = best_dx_for_bbox[idx, :]\n",
    "  y_rpn_regr *= config.std_scaling\n",
    "  return y_is_box_valid, y_rpn_overlap, y_rpn_regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1622982453589,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "CCt7ya1_EzHx"
   },
   "outputs": [],
   "source": [
    "def calc_rpn(config, img_data):\n",
    "  y_is_box_valid, y_rpn_overlap, y_rpn_regr = creat_anchorbox(config, img_data)\n",
    "  # this function will solve the issue RPN has many more negative anchor than positive anchor regions\n",
    "  # -> so we limit it to 256 regions by turn off some negative anchor region\n",
    "  regions_limit = 256\n",
    "  # Change the  form of input\n",
    "  '''\n",
    "  y_rpn_overlap, y_is_box_valid, y_rpn_regr (1, (position of anchor in feature map),anchor_idx)\n",
    "  '''\n",
    "  y_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=0)\n",
    "  y_is_box_valid = np.expand_dims(y_is_box_valid, axis=0)\n",
    "  y_rpn_regr = np.expand_dims(y_rpn_regr, axis=0)\n",
    " \n",
    "  # Determine the location of negative and positive anchor\n",
    "  pos_locs = np.where(np.logical_and(y_rpn_overlap[0,:, :, :] == 1, y_is_box_valid[0,:, :, :] == 1))\n",
    "  neg_locs = np.where(np.logical_and(y_rpn_overlap[0,:, :, :] == 0, y_is_box_valid[0,:, :, :] == 1))\n",
    "\n",
    "  num_pos = pos_locs[0].shape[0]\n",
    "  num_neg = neg_locs[0].shape[0]\n",
    "  if num_pos > ( regions_limit / 2 ):\n",
    "    val_locs = random.sample(range(num_pos), int(num_pos - regions_limit / 2))\n",
    "    y_is_box_valid[0, pos_locs[0][val_locs], pos_locs[1][val_locs], pos_locs[2][val_locs]] = 0\n",
    "    num_pos = int( regions_limit / 2)\n",
    "  if num_neg + num_pos > regions_limit:\n",
    "    val_locs = random.sample(range(num_neg),num_neg - num_pos)\n",
    "    y_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0\n",
    "\n",
    "\n",
    "  y_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis= -1)\n",
    "  y_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, 4, axis=-1), y_rpn_regr], axis= -1)\n",
    "  return y_rpn_cls,y_rpn_regr, num_pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaXX5k2zFeGj"
   },
   "source": [
    "## Generate the ground truth anchor for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1622982453590,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "Ah5cCXBXVaor"
   },
   "outputs": [],
   "source": [
    "def augment(img_data,config ,augment = True):\n",
    "  assert 'filepath' in img_data\n",
    "  assert 'height' in img_data\n",
    "  assert 'width' in img_data\n",
    "  assert 'bboxes' in img_data\n",
    "\n",
    "  img_data_aug = copy.deepcopy(img_data)\n",
    "  img = cv2.imread(img_data_aug['filepath'])\n",
    "\n",
    "  if augment:\n",
    "    height = img_data_aug['height']\n",
    "    width = img_data_aug['width']\n",
    "    if config.use_horizontal_flips and np.random.randint(0,2) == 0:\n",
    "      img = cv2.flip(img, 1)\n",
    "      # Change coordinate of bbox\n",
    "      for bbox in img_data_aug['bboxes']:\n",
    "        x1_temp = bbox['x1']\n",
    "        x2_temp = bbox['x2']\n",
    "        bbox['x1'] = width - x2_temp\n",
    "        bbox['x2'] = width - x1_temp\n",
    "    if config.use_vertical_flips and np.random.randint(0,2) == 0:\n",
    "      img = cv2.flip(img,0)\n",
    "      for bbox in img_data_aug['bboxes']:\n",
    "        y1_temp = bbox['y1']\n",
    "        y2_temp = bbox['y2']\n",
    "        bbox['y1'] = height - y2_temp\n",
    "        bbox['y2'] = height - y1_temp\n",
    "    if config.rot_90:\n",
    "      angle = np.random.choice([0,90,180,270], 1)[0]\n",
    "      if angle == 270:\n",
    "        img = np.transpose(img, (1, 0, 2))\n",
    "        img = cv2.flip(img, 0)\n",
    "      elif angle == 180:\n",
    "        img = cv2.flip(img, -1)\n",
    "      elif angle == 90:\n",
    "        img = np.transpose(img, (1, 0, 2))\n",
    "        img = cv2.flip(img, 1)\n",
    "      else:\n",
    "        pass\n",
    "      for bbox in img_data_aug['bboxes']:\n",
    "        x1 = bbox['x1']\n",
    "        x2 = bbox['x2']\n",
    "        y1 = bbox['y1']\n",
    "        y2 = bbox['y2']\n",
    "        if angle == 0:\n",
    "          pass\n",
    "        elif angle == 90:\n",
    "          bbox['x1'] = height - y2\n",
    "          bbox['x2'] = height - y1\n",
    "          bbox['y1'] = x1\n",
    "          bbox['y2'] = x2\n",
    "        elif angle == 180:\n",
    "          bbox['x1'] = width - x2\n",
    "          bbox['x2'] = width - x1\n",
    "          bbox['y1'] = height - y2\n",
    "          bbox['y2'] = height - y1\n",
    "        elif angle == 270:\n",
    "          bbox['x1'] = y1\n",
    "          bbox['x2'] = y2\n",
    "          bbox['y1'] = width - x2\n",
    "          bbox['y2'] = width - x1\n",
    "  img_data_aug['width'] = img.shape[1]\n",
    "  img_data_aug['height'] =img.shape[0]\n",
    "  return img_data_aug, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1622982453591,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "aFHg7EI5UVx2"
   },
   "outputs": [],
   "source": [
    "def get_new_img_size(img_data, img_min_size = 300):\n",
    "  '''\n",
    "  Resize image to the form size before training in network\n",
    "  '''\n",
    "  width = img_data['width']\n",
    "  height = img_data['height']\n",
    "  if width < height:\n",
    "    f = float(img_min_size / width)\n",
    "    resized_height = int(f * height)\n",
    "    resized_width = int(img_min_size)\n",
    "  else:\n",
    "    f = float(img_min_size / height)\n",
    "    resized_width = int(f * width)\n",
    "    resized_height = int(img_min_size)\n",
    "  # resize bboxes to fit new size of image\n",
    "  for bbox in img_data['bboxes']:\n",
    "    bbox['x1'] = int(bbox['x1'] * (resized_width / width))\n",
    "    bbox['x2'] = int(bbox['x2'] * (resized_width / width))\n",
    "    bbox['y1'] = int(bbox['y1'] * (resized_height / height))\n",
    "    bbox['y2'] = int(bbox['y2'] * (resized_height / height))\n",
    "  img_data['width'] = resized_width\n",
    "  img_data['height'] = resized_height\n",
    "  return  img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1622982453591,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "5dbcCGg2Fen8"
   },
   "outputs": [],
   "source": [
    "def get_anchor_gt(all_img_data, config, mode = 'train'):\n",
    "  '''\n",
    "  Yield the ground truth anchors as Y\n",
    "  Input:\n",
    "    all_img_data: list(filepath, width, height, list(bboxes))\n",
    "    config: config\n",
    "    mode: 'train' or 'test' < train mode need augemtation\n",
    "  output:\n",
    "    x_img: image data layer after resized and scaling (smallest size = 300 px)\n",
    "    Y:[y_rpn_cls, y_rpn_regr] \n",
    "    debug_img: a copy of imge for debug\n",
    "    num_pos: show the number of positive anchors for debug\n",
    "  '''\n",
    "  while True:\n",
    "    for img_data in all_img_data:\n",
    "      try:\n",
    "        # read image data and optionlly  add augmentation\n",
    "        if mode == 'train':\n",
    "          img_data_aug, x_img = augment(img_data, config, augment = True)\n",
    "        else:\n",
    "          img_data_aug, x_img = augment(img_data, config, augment = False)\n",
    "        height, width = x_img.shape[:2]\n",
    "        assert height == img_data_aug['height']\n",
    "        assert width == img_data_aug['width']\n",
    "        # resize image to the format input\n",
    "        img_data_aug = get_new_img_size(img_data_aug)\n",
    "        height = img_data_aug['height']\n",
    "        width  = img_data_aug['width']\n",
    "        x_img = cv2.resize(x_img, (width, height), interpolation=cv2.INTER_CUBIC)\n",
    "        debug_img = x_img.copy()\n",
    "        try:\n",
    "          y_rpn_cls, y_rpn_regr, num_pos = calc_rpn(config, img_data_aug)\n",
    "        except:\n",
    "          continue\n",
    "        # Zero-center by mean pixel, and preprocess image\n",
    "\n",
    "        x_img = x_img[:,:, (2, 1, 0)]  # BGR -> RGB\n",
    "        x_img = x_img.astype(np.float32)\n",
    "        x_img[:, :, 0] -= config.img_channel_mean[0]\n",
    "        x_img[:, :, 1] -= config.img_channel_mean[1]\n",
    "        x_img[:, :, 2] -= config.img_channel_mean[2]\n",
    "        x_img /= config.img_scaling_factor \n",
    "        x_img  = np.expand_dims(x_img, axis = 0)\n",
    "        \n",
    "        yield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug, debug_img, num_pos\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1622982453594,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "C_Obi4Zet1Hf"
   },
   "outputs": [],
   "source": [
    "# Get train data generator which generate X, Y, image_data\n",
    "data_gen_train = get_anchor_gt(all_data, config, mode = 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tY6sV49piw7l"
   },
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnQiL9kEi_Lb"
   },
   "source": [
    "## The base network (VGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1622982453595,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "-S--_Zb2JQs4"
   },
   "outputs": [],
   "source": [
    "def nn_base(input_tensor = None, trainable = False):\n",
    "  ## define convolution path of vgg network\n",
    "  input_shape =(None, None, 3)\n",
    "  if input_tensor is None:\n",
    "    input_shape = (None, None, 3)\n",
    "    img_input = Input(shape = input_shape)\n",
    "  else:\n",
    "    if not K.is_keras_tensor(input_tensor):\n",
    "      img_input = Input(tensor = input_tensor, shape = input_shape)\n",
    "    else:\n",
    "      img_input = input_tensor\n",
    "  bn_axis = 3\n",
    "  # Block 1\n",
    "  x = Conv2D( 64, (3, 3), activation = 'relu', padding = 'same', name = 'block1_conv1')(img_input)\n",
    "  x = Conv2D( 64, (3, 3), activation = 'relu', padding = 'same', name = 'block1_conv2')(x)\n",
    "  x = MaxPooling2D((2, 2), strides = (2, 2), name = 'block1_pool')(x)\n",
    "  # Block 2\n",
    "  x = Conv2D(128, (3, 3), activation = 'relu', padding = 'same', name = 'block2_conv1')(x)\n",
    "  x = Conv2D(128, (3, 3), activation = 'relu', padding = 'same', name = 'block2_conv2')(x)\n",
    "  x = MaxPooling2D((2, 2), strides = (2, 2), name = 'block2_pool')(x)\n",
    "  # Block 3\n",
    "  x = Conv2D(256, (3, 3), activation = 'relu', padding = 'same', name = 'block3_conv1')(x)\n",
    "  x = Conv2D(256, (3, 3), activation = 'relu', padding = 'same', name = 'block3_conv2')(x)\n",
    "  x = Conv2D(256, (3, 3), activation = 'relu', padding = 'same', name = 'block3_conv3')(x)\n",
    "  x = MaxPooling2D((2, 2), strides = (2, 2), name = 'block3_pool')(x)\n",
    "  # Block 4\n",
    "  x = Conv2D(512, (3, 3), activation = 'relu', padding = 'same', name = 'block4_conv1')(x)\n",
    "  x = Conv2D(512, (3, 3), activation = 'relu', padding = 'same', name = 'block4_conv2')(x)\n",
    "  x = Conv2D(512, (3, 3), activation = 'relu', padding = 'same', name = 'block4_conv3')(x)\n",
    "  x = MaxPooling2D((2, 2), strides = (2, 2), name = 'block4_pool')(x)\n",
    "  # Block 5\n",
    "  x = Conv2D(512, (3, 3), activation = 'relu', padding = 'same', name = 'block5_conv1')(x)\n",
    "  x = Conv2D(512, (3, 3), activation = 'relu', padding = 'same', name = 'block5_conv2')(x)\n",
    "  x = Conv2D(512, (3, 3), activation = 'relu', padding = 'same', name = 'block5_conv3')(x)\n",
    "  # x = MaxPooling2D((2, 2), strides = (2, 2), name = 'block5_pool')(x) \n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWn6efQr2_di"
   },
   "source": [
    "## RPN layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRG4YD7TFIUd"
   },
   "source": [
    "Loss Function\n",
    "  <Use the formala in paper fast - rcnn>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1622982453596,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "t0iONr3F8Weo"
   },
   "outputs": [],
   "source": [
    "def rpn_layer(base_layers, num_anchors):\n",
    "  '''\n",
    "  Creat a rpn layer\n",
    "    Step 1: Pass throught feature map from base layer to a (3, 3, 512) chanels convolutional layer\n",
    "            keep the padding \"same\" to preserve the feature map size\n",
    "    Step 2: Pass the step 1 to two (1, 1) convolution layer to replacw the fully connected layer\n",
    "    classification layer: num_anchors channels for signmoid activate functions\n",
    "    regression layer: num_anchors channels * 4 channel for compute the regression pf bboxes with linear activatet\n",
    "  input:\n",
    "    base_layers; vgg16\n",
    "    num_anchors: number of anchors\n",
    "  output:\n",
    "    x_class: classification for whether it is a object\n",
    "    x_regr: regression for bboxes\n",
    "    base_layers\n",
    "  '''\n",
    "  x = Conv2D(512, (3, 3), padding = 'same', activation = 'relu', kernel_initializer = 'normal', name = 'rpn_conv1')(base_layers)\n",
    "  x_class = Conv2D(num_anchors, (1, 1), activation = 'sigmoid',  kernel_initializer = 'uniform',name = 'rpn_out_class')(x)\n",
    "  x_regr  = Conv2D(num_anchors * 4, (1, 1),activation = 'linear',kernel_initializer= 'zero',   name =  'rpn_out_regr' )(x)\n",
    "  return [x_class, x_regr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1622982453596,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "Xj_d4ollFNXN"
   },
   "outputs": [],
   "source": [
    "def rpn_loss_regr(num_anchors):\n",
    "    \"\"\"Loss function for rpn regression\n",
    "    Args:\n",
    "        num_anchors: number of anchors (9 in here)\n",
    "    Returns:\n",
    "        Smooth L1 loss function \n",
    "                           0.5*x*x (if x_abs < 1)\n",
    "                           x_abx - 0.5 (otherwise)\n",
    "    \"\"\"\n",
    "    lambda_rpn_regr = 1.0\n",
    "    epsilon = 1e-4\n",
    "    def rpn_loss_regr_fixed_num(y_true, y_pred):\n",
    "\n",
    "        # x is the difference between true value and predicted vaue\n",
    "        x = y_true[:, :, :, 4 * num_anchors:] - y_pred\n",
    "\n",
    "        # absolute value of x\n",
    "        x_abs = K.abs(x)\n",
    "\n",
    "        # If x_abs <= 1.0, x_bool = 1\n",
    "        x_bool = K.cast(K.less_equal(x_abs, 1.0), tf.float32)\n",
    "\n",
    "        return lambda_rpn_regr * K.sum(\n",
    "            y_true[:, :, :, :4 * num_anchors] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :, :4 * num_anchors])\n",
    "\n",
    "    return rpn_loss_regr_fixed_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1622982453597,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "eSCMHgiCVM8P"
   },
   "outputs": [],
   "source": [
    "def rpn_loss_cls(num_anchors):\n",
    "    \"\"\"Loss function for rpn classification\n",
    "    Args:\n",
    "        num_anchors: number of anchors (9 in here)\n",
    "        y_true[:, :, :, :9]: [0,1,0,0,0,0,0,1,0] means only the second and the eighth box is valid which contains pos or neg anchor => isValid\n",
    "        y_true[:, :, :, 9:]: [0,1,0,0,0,0,0,0,0] means the second box is pos and eighth box is negative\n",
    "    Returns:\n",
    "        lambda * sum((binary_crossentropy(isValid*y_pred,y_true))) / N\n",
    "    \"\"\"\n",
    "    epsilon = 1e-4\n",
    "    lambda_rpn_class = 1\n",
    "    def rpn_loss_cls_fixed_num(y_true, y_pred):\n",
    "\n",
    "            return lambda_rpn_class * K.sum(y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / K.sum(epsilon + y_true[:, :, :, :num_anchors])\n",
    "\n",
    "    return rpn_loss_cls_fixed_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPweV9gHqrNW"
   },
   "source": [
    "Start build RPN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4534,
     "status": "ok",
     "timestamp": 1622982458114,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "UJUssQOKqqkT",
    "outputId": "d68c580b-fd8a-4885-ff01-ad19b55ce116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, None, None, 6 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, None, None, 6 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, None, None, 6 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, None, None, 1 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, None, None, 1 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, None, None, 1 0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, None, None, 2 295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, None, None, 2 590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, None, None, 2 590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, None, None, 2 0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, None, None, 5 1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, None, None, 5 2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, None, None, 5 2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, None, None, 5 0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, None, None, 5 2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, None, None, 5 2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, None, None, 5 2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "rpn_conv1 (Conv2D)              (None, None, None, 5 2359808     block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "rpn_out_class (Conv2D)          (None, None, None, 9 4617        rpn_conv1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "rpn_out_regr (Conv2D)           (None, None, None, 3 18468       rpn_conv1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 17,097,581\n",
      "Trainable params: 17,097,581\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape_img = (None, None, 3)\n",
    "img_input = Input(shape = input_shape_img)\n",
    "shared_layers = nn_base(img_input, trainable = True)\n",
    "rpn =rpn_layer(shared_layers, 9)\n",
    "model_rpn = Model(inputs = img_input,outputs = rpn)\n",
    "rpn_optimizer =Adam(lr = 1e-5)\n",
    "model_rpn.compile(optimizer=rpn_optimizer, loss=[rpn_loss_cls(9), rpn_loss_regr(9)])\n",
    "print(model_rpn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1622982458116,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "aezmx1TXFiWU"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXWV9jjDIWce"
   },
   "source": [
    "## Compute RPN to ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1622982459058,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "tzj2HPBFkHx7"
   },
   "outputs": [],
   "source": [
    "def non_max_suppression(boxes, probs, overlap_thresh = 0.9, max_boxes = 300):\n",
    "  '''\n",
    "  use to remove reduncant bboxes\n",
    "  input:\n",
    "    boxes: coordinate of bboxes (it might be (x1, y1, x2, y2))\n",
    "    probs: rpn_layer \n",
    "  output:\n",
    "    bboxes set\n",
    "  '''\n",
    "  if boxes.shape[0] == 0:\n",
    "    return []\n",
    "  # grab the coordinate of the bboxes\n",
    "  x1 = boxes[:, 0]\n",
    "  x2 = boxes[:, 2]\n",
    "  y1 = boxes[:, 1]\n",
    "  y2 = boxes[:, 3]\n",
    "  # assert condition of x1, x2, y1, y2\n",
    "  np.testing.assert_array_less(x1, x2)\n",
    "  np.testing.assert_array_less(y1, y2)\n",
    "  # convert integer coordinate to the float coordinate\n",
    "  if boxes.dtype.kind == \"i\":\n",
    "    boxes = boxes.astype(\"float\")\n",
    "  # initialize the list of picked indexes\n",
    "  pick = []\n",
    "  # calculate the areas\n",
    "  # Note that we cant use the IOU function above beacause this input is array \n",
    "  area = (x2 - x1) * (y2 - y1)\n",
    "  # Sort the bounding box\n",
    "  idxs = np.argsort(probs)\n",
    "  while len(idxs) > 0:\n",
    "    # grab the last index ( the max confidence score)  and add the index value to the pick list\n",
    "    last = len(idxs) - 1\n",
    "    i = idxs[last]\n",
    "    pick.append(i)\n",
    "    # find the intersection when compare [i] and others\n",
    "    xmin_i = np.maximum(x1[i], x1[idxs[:last]])\n",
    "    xmax_i = np.minimum(x2[i], x2[idxs[:last]])\n",
    "    ymin_i = np.maximum(y1[i], y1[idxs[:last]])\n",
    "    ymax_i = np.minimum(y2[i], y2[idxs[:last]])\n",
    "    # compute width, height and area of intersection\n",
    "    width  = np.maximum(0,xmax_i - xmin_i)\n",
    "    height = np.maximum(0,ymax_i - ymin_i)\n",
    "    \n",
    "    area_i = width * height\n",
    "    # compute the union\n",
    "    area_u = area[i] + area[idxs[:last]] - area_i\n",
    "    # compute the overlap by iou\n",
    "    overlap= area_i/(area_u + 1e-6)\n",
    "    # deleta all indexes from the index thas have overlap > threshold\n",
    "    idxs = np.delete(idxs,np.concatenate(([last], np.where(overlap > overlap_thresh)[0])))\n",
    "    if len(pick) >= max_boxes:\n",
    "      break\n",
    "  # return only the bboxes that were picked, data return is integer\n",
    "  boxes = boxes[pick].astype(\"int\")\n",
    "  probs  = probs[pick]\n",
    "  boxes = boxes[:,(0, 2, 1,3)] # x1, y1, x2. y2 --> x1, x2, y1, y2\n",
    "  return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1622982459058,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "xPFbtDfSZoLh"
   },
   "outputs": [],
   "source": [
    "def apply_regr(X, T):\n",
    "  '''\n",
    "  apply regression layer to all anchors in one features map\n",
    "  input:\n",
    "    X (num_coordinate(4), height , width): the current anchor type for all point in feature map\n",
    "    T (num_coordinate(4), height , width): tge regression layer output [tx, ty ,th, tw]\n",
    "    return \n",
    "    X: regressed\n",
    "  '''\n",
    "  try:\n",
    "    x  = X[0, :, :]\n",
    "    y  = X[1, :, :]\n",
    "    w  = X[2, :, :]\n",
    "    h  = X[3, :, :]\n",
    "    tx  = T[0, :, :]\n",
    "    ty  = T[1, :, :]\n",
    "    tw  = T[2, :, :]\n",
    "    th  = T[3, :, :]\n",
    "    # compute the box center coordinate by use [2] in paper\n",
    "    cx = x + w / 2\n",
    "    cy = y + h / 2\n",
    "    cx1 = tx * w + cx\n",
    "    cy1 = tx * w + cy\n",
    "    w1 = np.exp(tw.astype(np.float128)) * w\n",
    "    h1 = np.exp(th.astype(np.float128)) * h\n",
    "    x1 = cx1 - w1 / 2\n",
    "    y1 = cy1 - h1 / 2\n",
    "\n",
    "    x1 = np.round(x1)\n",
    "    y1 = np.round(y1)\n",
    "    w1 = np.round(w1)\n",
    "    h1 = np.round(h1)\n",
    "    return np.stack([x1, y1, w1, h1])\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1622982459059,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "x-ApXd_mIcBT"
   },
   "outputs": [],
   "source": [
    "def rpn_to_roi(rpn_layer, regr_layer, config, dim_ordering = K.image_data_format() , use_regr = False, max_bboxes =300, overlap_thresh = 0.9):\n",
    "  '''\n",
    "  Convert rpn layer to roi bboxes before pass in classifier\n",
    "  input:\n",
    "    rpn_layer (1, feature map height, feature map width, num anchor): result from rpn classification it include (y_rpn_overlap abd y_is_box_valid)\n",
    "    regr_layer (1, feature map height, feature map width, num anchor * 4): output result from rpn regression : one element like [tx, ty, tw, th] and y_rpn_overlap which show the positive or negative proposed\n",
    "    config \n",
    "    use_regr: wether to use bboxes regression in rpn\n",
    "    max_bboxes: max bboxes number for non max suppression\n",
    "    overlap_thresh: if iou in NMS larger than this thresshold, drop this bbox\n",
    "  output:\n",
    "    result: bboxes from non max suppression (on feature map)\n",
    "  '''\n",
    "  assert rpn_layer.shape[0] == 1\n",
    "  regr_layer /= config.std_scaling\n",
    "  (height, width) = rpn_layer.shape[1:3] # height, width in the feature map\n",
    "  anchor_sizes = config.anchor_box_scales\n",
    "  anchor_ratios= config.anchor_box_ratios\n",
    "  curr_layer = 0\n",
    "  # creat a temp array which is coordinates for 9 anchors for everypoint in feature maps\n",
    "  # shape: (4, height of feature map, width of feature map, num_anchors)\n",
    "  # E.x: (4, 18, 25, 18)\n",
    "  A = np.zeros((4, rpn_layer.shape[1], rpn_layer.shape[2], rpn_layer.shape[3]))\n",
    "  for size_idx in range (len(anchor_sizes)):\n",
    "    for ratio_idx in range (len(anchor_ratios)):\n",
    "      # compute width and height of anchor in the feature map\n",
    "      anchor_width = (anchor_sizes[size_idx] * anchor_ratios[ratio_idx][0]) / config.rpn_stride\n",
    "      anchor_height= (anchor_sizes[size_idx] * anchor_ratios[ratio_idx][1]) / config.rpn_stride\n",
    "      # we have 9 anchors in each position -> with the k-th anchors of all position in the feature map\n",
    "      regr = regr_layer[0, :, :, 4 * curr_layer: (4 * curr_layer + 4)] # (heght, width, [tx, ty, tw, th])\n",
    "      regr = np.transpose(regr, (2, 0, 1)) # ([4, height, width])\n",
    "      # creat a mesh grid, creat each point (X, Y)\n",
    "      X, Y = np.meshgrid(np.arange(width), np.arange(height))\n",
    "      # calculate anchor position and size for each feature map point (note: need to fit the format)\n",
    "      A[0, :, :, curr_layer] = X - anchor_width / 2\n",
    "      A[1, :, :, curr_layer] = Y - anchor_height / 2    \n",
    "      A[2, :, :, curr_layer] = anchor_width\n",
    "      A[3, :, :, curr_layer] = anchor_height\n",
    "      # Apply regression layer to x, y ,w ,h if there is rpn regression layer\n",
    "      if use_regr:\n",
    "        A[:,:, :, curr_layer] = apply_regr(A[:, :, : , curr_layer], regr)\n",
    "      # Avoid width and height small than 1\n",
    "      A[2, :, :, curr_layer] = np.maximum(1, A[2, :, :, curr_layer])\n",
    "      A[3, :, :, curr_layer] = np.maximum(1, A[3, :, :, curr_layer])\n",
    "      # convert (x1, y1, w, h) to (x1, y1, x2, y2)\n",
    "      A[2, :, :, curr_layer] += A[0, :, :, curr_layer] \n",
    "      A[3, :, :, curr_layer] += A[1, :, :, curr_layer]\n",
    "      # avoid bboxes outside the feature mao\n",
    "      A[0, :, :, curr_layer] = np.maximum(0, A[0, :, :, curr_layer])\n",
    "      A[1, :, :, curr_layer] = np.maximum(0, A[1, :, :, curr_layer])   \n",
    "      A[2, :, :, curr_layer] = np.minimum(width - 1, A[2, :, :, curr_layer]) \n",
    "      A[3, :, :, curr_layer] = np.minimum(height - 1, A[3, :, :, curr_layer]) \n",
    "      curr_layer += 1\n",
    "  # reshape to fit the format\n",
    "  A = A.transpose((0, 3, 1, 2)) # (1, num_anchors, heght, width)\n",
    "  all_boxes = np.reshape(A,(4, -1)).transpose(1, 0) # (widh * height * num_anchors, 4)\n",
    "  all_probs = rpn_layer.transpose((0, 3, 2, 1)).reshape(-1) # like above, shape (width * height * num_anchor, )\n",
    "  x1 = all_boxes[:, 0]\n",
    "  y1 = all_boxes[:, 1]\n",
    "  x2 = all_boxes[:, 2]\n",
    "  y2 = all_boxes[:, 3]\n",
    "  # find out the bboxes which is illegal and delete it from the list\n",
    "  idxs = np.where(((x1 - x2 >= 0) | (y1 - y2 >= 0)))\n",
    "  all_boxes = np.delete(all_boxes, idxs, 0)\n",
    "  all_probs = np.delete(all_probs, idxs, 0)\n",
    "  # Apply non_max_suppression\n",
    "  boxes = non_max_suppression(all_boxes, all_probs, overlap_thresh = overlap_thresh, max_boxes = max_bboxes)\n",
    "  return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 661,
     "status": "ok",
     "timestamp": 1622982459695,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "6hSX6QiBIwK6"
   },
   "outputs": [],
   "source": [
    "def convert_coor(bboxes, img_data, config, class_mapping):\n",
    "  '''\n",
    "  converts (x1, x2, y1, y2) to (x, y, w, h) (x, y) is the min coordinates of (X, Y) and output of NN\n",
    "  input:\n",
    "    boxes[num_bboxes,4]: is bbox results of the converting RPN to ROI process\n",
    "    img_data: information of image\n",
    "    config\n",
    "    class_mapping: order of class\n",
    "  output:\n",
    "    X2: the format coordinate of bboxes\n",
    "    Y1: one hot code for bboxes from aboves \n",
    "    Y2: the output for the regression labels and coordinate\n",
    "  '''\n",
    "  # ground true box\n",
    "  gtb = np.zeros((len(img_data['bboxes']), 4))\n",
    "  for i, bbox in enumerate (img_data['bboxes']):\n",
    "    gtb[i, 0] = int( bbox['x1'] / config.rpn_stride)\n",
    "    gtb[i, 1] = int( bbox['x2'] / config.rpn_stride)\n",
    "    gtb[i, 2] = int( bbox['y1'] / config.rpn_stride)\n",
    "    gtb[i, 3] = int( bbox['y2'] / config.rpn_stride)\n",
    "  # height, width\n",
    "  height = img_data['height']\n",
    "  width = img_data['width']\n",
    "  #\n",
    "  x_roi = [] # coordinates of bbox is like [x, y, w, h]\n",
    "  y_class_num =[] # save the output for classifier label\n",
    "  y_class_regr_coords = [] # save the output for the regression coordinate\n",
    "  y_class_regr_labels = [] # save the output for the regression labels\n",
    "  IoUs = []\n",
    "  # array of coordinate bbox\n",
    "  num_bboxes = np.array(bboxes).shape[0] # number of bboxes\n",
    "  # standart devation of classifier regression\n",
    "  sx, sy, sw, sh = config.classifier_regr_std\n",
    "  for ix in range (num_bboxes):\n",
    "    x1 =int(np.round(bboxes[ix, 0]))\n",
    "    x2 =int(np.round(bboxes[ix, 1]))\n",
    "    y1 =int(np.round(bboxes[ix, 2]))\n",
    "    y2 =int(np.round(bboxes[ix, 3]))\n",
    "    # compute each IoU for ground truth box\n",
    "    best_iou =  0  # save the value of iou of this bbox and all ground truth box\n",
    "    best_box = -1  # save the idx of ground truth box\n",
    "    # itterate throught all the ground truth boxxes to compute iou\n",
    "    for num_bbox in range (gtb.shape[0]):\n",
    "      # current Iou\n",
    "      curr_iou =iou(gtb[num_bbox, :], [x1, x2, y1, y2])\n",
    "      # find the ground truth box which is the largest IOU with this bbox, if it is max, may be bboxes belong to ground truth box\n",
    "      if curr_iou > best_iou:\n",
    "        best_iou = curr_iou\n",
    "        best_box = num_bbox\n",
    "    # after we have a ground truth box fit to the bbox\n",
    "    # if value is so small we need to skip it because the propability it belong to ground truth box is very small\n",
    "    if best_iou < config.classifier_min_overlap:\n",
    "      continue\n",
    "    else:\n",
    "      # we compute the format coordinate (fast RCNN paper)\n",
    "      wa = x2 - x1\n",
    "      ha = y2 - y1\n",
    "      x_roi.append([x1, y1, wa, ha])\n",
    "      IoUs.append(best_iou)\n",
    "      if   best_iou < config.classifier_max_overlap:\n",
    "        # -> it is background\n",
    "        cls_name = 'bg'\n",
    "      elif best_iou >= config.classifier_max_overlap:\n",
    "        cls_name = img_data['bboxes'][best_box]['class']\n",
    "        # center of ground truth box\n",
    "        cx = (gtb[best_box, 0] + gtb[best_box, 1])/ 2\n",
    "        cy = (gtb[best_box, 2] + gtb[best_box, 2])/ 2\n",
    "        # center of bbox\n",
    "        cxa = (x1 + x2) / 2\n",
    "        cya = (y1 + y2) / 2\n",
    "        # the parameterization\n",
    "        tx = (cx - cxa) / float(wa + 1e-6)\n",
    "        ty = (cy - cya) / float(ha + 1e-6)\n",
    "        tw = np.log((gtb[best_box, 1] - gtb[best_box, 0]) / float(wa + 1e-6))\n",
    "        th = np.log((gtb[best_box, 3] - gtb[best_box, 2]) / float(ha + 1e-6))\n",
    "      else:\n",
    "        raise RuntimeError\n",
    "    # creat the final output of softmax\n",
    "    class_num   = class_mapping[cls_name] # order location of class in set\n",
    "    class_label = np.zeros(len(class_mapping))\n",
    "    class_label[class_num] = 1\n",
    "    y_class_num.append(copy.deepcopy(class_label))\n",
    "    # creat the final output for bboxes coordinates regression\n",
    "    coords = np.zeros(4 * (len(class_mapping) - 1 )) # coordinate\n",
    "    labels = np.zeros(4 * (len(class_mapping) - 1 ))\n",
    "    if cls_name != 'bg':\n",
    "      start = 4 * class_num\n",
    "      coords[start:start + 4] = [sx * tx, sy * ty, sw * tw, sh * th]\n",
    "      labels[start:start + 4] = [1, 1, 1, 1]\n",
    "      y_class_regr_coords.append(coords)\n",
    "      y_class_regr_labels.append(labels)\n",
    "    else:\n",
    "      y_class_regr_coords.append(coords)\n",
    "      y_class_regr_labels.append(labels)\n",
    "  if len(x_roi ) == 0:\n",
    "    return None, None, None, None\n",
    "  # X2\n",
    "  X = np.array(x_roi)\n",
    "  # one hot code for bboxes from aboves => x_roi(x)\n",
    "  Y1 = np.array(y_class_num)\n",
    "  # correspoing label and corresponding gt bboxes\n",
    "  Y2 = np.concatenate((np.array(y_class_regr_labels), np.array(y_class_regr_coords)), axis = 1)\n",
    "  return np.expand_dims(X, axis = 0), np.expand_dims(Y1, axis = 0), np.expand_dims(Y2, axis = 0), IoUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WCHwUC5xk_o"
   },
   "source": [
    "## ROIPoolinglayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1622982459696,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "fahvLJm1xp1v"
   },
   "outputs": [],
   "source": [
    "class RoiPoolingConv(Layer):\n",
    "  '''Roi pooling layer for 2D input\n",
    "  arg:\n",
    "    pool_size (int):  Size of the pooling region to use. If poolsize = 7, result will be an 7x7 region\n",
    "    num_rois: number of regions of interest to be used\n",
    "  input shape:\n",
    "    list of two 4D tensors [X_img, X_roi] with shape:\n",
    "    X_img (1, rows, cols, channel)\n",
    "    X_roi (1, num_rois, 4):  list of rois, with ordering ((x, y,w, h))\n",
    "  output_shape\n",
    "    3D tensor with shape (1, num_rois, channels, pool_size, pool_size)\n",
    "  '''\n",
    "  def __init__(self, pool_size, num_rois, **kwargs):\n",
    "    self.dim_ordering = K.image_data_format() \n",
    "    self.pool_size = pool_size\n",
    "    self.num_rois = num_rois\n",
    "    super(RoiPoolingConv, self).__init__(**kwargs)\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.nb_channels = input_shape[0][3]\n",
    "\n",
    "  def compute_output_shape(self, input_shape):\n",
    "    return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channel\n",
    "  \n",
    "  def call(self, x, mask = None):\n",
    "    assert len(x) == 2\n",
    "    # x[0] is image with shape (1, rows, cols, channels)\n",
    "    img = x[0]\n",
    "    # x[1] is roi with shape (1, num_rois, 4), with (num_roi_idx, :) =[ x. y. w. h]\n",
    "    rois = x[1]\n",
    "    input_shape = K.shape(img)\n",
    "    outputs = []\n",
    "    for roi_idx in range (self.num_rois):\n",
    "      x = rois[0, roi_idx, 0]\n",
    "      y = rois[0, roi_idx, 1]\n",
    "      w = rois[0, roi_idx, 2]\n",
    "      h = rois[0, roi_idx, 3]\n",
    "      # convert tensor with d_type (int)\n",
    "      x = K.cast(x, 'int32')\n",
    "      y = K.cast(y, 'int32')\n",
    "      w = K.cast(w, 'int32')\n",
    "      h = K.cast(h, 'int32')\n",
    "      # resized roi of image to pooling size ( 7x7)\n",
    "      # Note that: (y:y+h) and (x: x+w) is set of pixel which contain regions of interest\n",
    "      rs = tf.image.resize(img[:,y:y+h, x:x + w, :],(self.pool_size,self.pool_size))\n",
    "      # rs shape might be (1, pool_size, pool_size, 3)\n",
    "      outputs.append(rs)\n",
    "    # The size of output might be (num_rois, pool_size, pool_size, 3)\n",
    "    final_output = K.concatenate(outputs, axis = 0)\n",
    "    # -> after concatenate the shape might be (num_rois * pool_size, pool_size, 3)\n",
    "    # -> reshape to ( 1, num_rois, pool_size, pool_size, 3)\n",
    "    final_output = K.reshape(final_output,(1, self.num_rois, self.pool_size, self.pool_size,self.nb_channels))\n",
    "    # permute dimession is similar to transpose\n",
    "    final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n",
    "    return final_output\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {'pool_size': self.pool_size,\n",
    "              'num_rois' : self.num_rois}\n",
    "    base_config = super(RoiPoolingConv, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOtHiBWZ78Qw"
   },
   "source": [
    "## Classify Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1622982459697,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "ZuPCTEoO8C0i"
   },
   "outputs": [],
   "source": [
    "def classifier_layer(base_layers, input_rois, num_rois, nb_classes = 4):\n",
    "  '''\n",
    "  Create a classifier layer\n",
    "  input:\n",
    "    base_layers: results after vgg\n",
    "    input_rois (1, num_rois, 4): which is list of rois, with odering (x, y, w, h)\n",
    "    num_rois: number of rois to be processed in one time (4)\n",
    "  output:\n",
    "    list(out_class, out_regr): see more details in paper of fast rcnn\n",
    "    out_class: classifier layer output\n",
    "    out_regr : regression layer output \n",
    "  '''\n",
    "  input_shape = (num_rois, 7, 7, 512)\n",
    "  pooling_regions = 7\n",
    "  # out_roi_pool.shape = (1, num_rois, pool_size, pool_size, nb_channels)\n",
    "  roi_pooling_conv = RoiPoolingConv(pooling_regions, num_rois)\n",
    "  roi_pooling_layer = roi_pooling_conv([base_layers, input_rois])\n",
    "  # Flatten the convolutional layer and  connected to 2 Fully connected layer and 2 dropout\n",
    "  out = TimeDistributed(Flatten(name = 'flatten'))(roi_pooling_layer)\n",
    "  out = TimeDistributed(Dense(4096, activation = 'relu', name = 'fc1'))(out)\n",
    "  out = TimeDistributed(Dropout(0.5))(out)\n",
    "  out = TimeDistributed(Dense(4096, activation = 'relu', name = 'fc2'))(out)\n",
    "  out = TimeDistributed(Dropout(0.5))(out)\n",
    "  # We compute 2 output layer\n",
    "  # out_class: softmax activation function for classify the class name of the object\n",
    "  # out_regr: linear activation function for bboxes coordinates regression\n",
    "  out_class = TimeDistributed(Dense(nb_classes, activation = 'softmax', kernel_initializer = 'zero'), name = 'dense_class_{}'.format(nb_classes))(out)\n",
    "  out_regr  = TimeDistributed(Dense(4 * (nb_classes -1), activation = 'linear',kernel_initializer = 'zero'), name = 'dense_regress_{}'.format(nb_classes))(out)\n",
    "  # nb_classes -1 because we dont compute regr for background class\n",
    "  return [out_class, out_regr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JC2YPwBcZdky"
   },
   "source": [
    " Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1622982459698,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "linCzesuZgoD"
   },
   "outputs": [],
   "source": [
    "def class_loss_regr(num_classes):\n",
    "    \"\"\"Loss function for rpn regression\n",
    "    Args:\n",
    "        num_anchors: number of anchors (9 in here)\n",
    "    Returns:\n",
    "        Smooth L1 loss function \n",
    "                           0.5*x*x (if x_abs < 1)\n",
    "                           x_abx - 0.5 (otherwise)\n",
    "    \"\"\"\n",
    "    epsilon = 1e-4\n",
    "    lambda_cls_regr = 1.0\n",
    "    def class_loss_regr_fixed_num(y_true, y_pred):\n",
    "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
    "        x_abs = K.abs(x)\n",
    "        x_bool = K.cast(K.less_equal(x_abs, 1.0), 'float32')\n",
    "        return lambda_cls_regr * K.sum(y_true[:, :, :4*num_classes] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :4*num_classes])\n",
    "    return class_loss_regr_fixed_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1622982459699,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "gU73KpIpbGS9"
   },
   "outputs": [],
   "source": [
    "def class_loss_cls(y_true, y_pred):\n",
    "    lambda_cls_class = 1.0\n",
    "    return lambda_cls_class * K.mean(categorical_crossentropy(y_true[0, :, :], y_pred[0, :, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrxVJMHlvxzF"
   },
   "source": [
    "Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1622982459700,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "QUKyxHrRvzJc",
    "outputId": "792795ac-07d9-4109-eca5-bec9d9606df1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, None, None, 6 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, None, None, 6 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, None, None, 6 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, None, None, 1 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, None, None, 1 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, None, None, 1 0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, None, None, 2 295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, None, None, 2 590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, None, None, 2 590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, None, None, 2 0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, None, None, 5 1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, None, None, 5 2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, None, None, 5 2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, None, None, 5 0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, None, None, 5 2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, None, None, 5 2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, None, None, 5 2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 4)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "roi_pooling_conv (RoiPoolingCon (1, 4, 7, 7, 512)    0           block5_conv3[0][0]               \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (1, 4, 25088)        0           roi_pooling_conv[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (1, 4, 4096)         102764544   time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (1, 4, 4096)         0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (1, 4, 4096)         16781312    time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (1, 4, 4096)         0           time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_class_4 (TimeDistributed) (1, 4, 4)            16388       time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_regress_4 (TimeDistribute (1, 4, 12)           49164       time_distributed_4[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 134,326,096\n",
      "Trainable params: 134,326,096\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "roi_input = Input(shape=(None, 4))\n",
    "classifier = classifier_layer(shared_layers, roi_input, config.num_rois, nb_classes=len(classes_count))\n",
    "model_classifier = Model(inputs = [img_input, roi_input],outputs = classifier)\n",
    "classifier_optimizer = Adam(lr = 1e-5)\n",
    "model_classifier.compile(optimizer=classifier_optimizer, loss=[class_loss_cls, class_loss_regr(len(classes_count)-1)], metrics={'dense_class_{}'.format(len(classes_count)): 'accuracy'})\n",
    "\n",
    "print(model_classifier.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9x7BS5VIp3b-"
   },
   "source": [
    "# Prepare for trainning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcoYsuCGw2zA"
   },
   "source": [
    "**Build Model All to easy load weight**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1622982461118,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "kbJ9YNKwpwJD",
    "outputId": "3473e373-9deb-4782-a0af-d5c8295284cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, None, None, 6 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, None, None, 6 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, None, None, 6 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, None, None, 1 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, None, None, 1 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, None, None, 1 0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, None, None, 2 295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, None, None, 2 590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, None, None, 2 590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, None, None, 2 0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, None, None, 5 1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, None, None, 5 2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, None, None, 5 2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, None, None, 5 0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, None, None, 5 2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, None, None, 5 2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, None, None, 5 2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 4)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "roi_pooling_conv (RoiPoolingCon (1, 4, 7, 7, 512)    0           block5_conv3[0][0]               \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (1, 4, 25088)        0           roi_pooling_conv[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (1, 4, 4096)         102764544   time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (1, 4, 4096)         0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (1, 4, 4096)         16781312    time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "rpn_conv1 (Conv2D)              (None, None, None, 5 2359808     block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (1, 4, 4096)         0           time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "rpn_out_class (Conv2D)          (None, None, None, 9 4617        rpn_conv1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "rpn_out_regr (Conv2D)           (None, None, None, 3 18468       rpn_conv1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_class_4 (TimeDistributed) (1, 4, 4)            16388       time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_regress_4 (TimeDistribute (1, 4, 12)           49164       time_distributed_4[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 136,708,989\n",
      "Trainable params: 136,708,989\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_all = Model(inputs = [img_input, roi_input], outputs = rpn + classifier)\n",
    "model_all.compile(optimizer='sgd', loss='mae')\n",
    "print(model_all.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Dshw4I8xbH8"
   },
   "source": [
    "**Load weight to continue train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3346,
     "status": "ok",
     "timestamp": 1622982464453,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "92Fe0iESndaU",
    "outputId": "e4734399-837b-4a72-c235-14a2599a8348"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continue training based aon the previous trained model\n",
      "loading weights from model/model_faster_rcnn.h5\n",
      "Already train 47K batches\n",
      "we have train:  47  total epoch\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(config.model_path):\n",
    "  # if we dont have pretrain file\n",
    "  if not os.path.isfile(config.base_net_weights):\n",
    "    try:\n",
    "      print('download pre-train file from Keras github')\n",
    "      WEIGHTS_PATH = ('https://storage.googleapis.com/tensorflow/keras-applications/'\n",
    "                'vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "      weights_path =data_utils.get_file(\n",
    "          'vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "          WEIGHTS_PATH,\n",
    "          cache_subdir='models',\n",
    "          file_hash='64373286793e3c8b2b4e3219cbf3544b')\n",
    "      print(\"Trainning the first time\")\n",
    "      print(\"Loading weights from {}\".format(weights_path))\n",
    "      model_rpn.load_weights(weights_path)\n",
    "      model_classifier.load_weights(weights_path)\n",
    "    except:\n",
    "      print('Can not load pretrained model weight')\n",
    "  # if we have a pretrained file\n",
    "  else:\n",
    "    print(\"Trainning the first time\")\n",
    "    print(\"Loading weights from {}\".format(config.base_net_weights))\n",
    "    model_rpn.load_weights(config.base_net_weights, by_name = True)\n",
    "    model_classifier.load_weights(config.base_net_weights, by_name = True)\n",
    "    record_df = pd.DataFrame(columns=['mean_overlapping_bboxes', 'class_acc', 'loss_rpn_cls', 'loss_rpn_regr', 'loss_class_cls', 'loss_class_regr', 'curr_loss', 'elapsed_time', 'mAP'])\n",
    "else:\n",
    "  # continue trainning, load the trainned model from before \n",
    "  print('continue training based aon the previous trained model')\n",
    "  print('loading weights from {}'.format(config.model_path))\n",
    "  model_rpn.load_weights(config.model_path, by_name = True)\n",
    "  model_classifier.load_weights(config.model_path, by_name = True)\n",
    "\n",
    "  # load the record\n",
    "  record_df = pd.read_csv(record_path)\n",
    "\n",
    "  r_mean_overlapping_bboxes = record_df['mean_overlapping_bboxes']\n",
    "  r_class_acc = record_df['class_acc']\n",
    "  r_loss_rpn_cls = record_df['loss_rpn_cls']\n",
    "  r_loss_rpn_regr = record_df['loss_rpn_regr']\n",
    "  r_loss_class_cls = record_df['loss_class_cls']\n",
    "  r_loss_class_regr = record_df['loss_class_regr']\n",
    "  r_curr_loss = record_df['curr_loss']\n",
    "  r_elapsed_time = record_df['elapsed_time']\n",
    "  r_mAP = record_df['mAP']\n",
    "  print('Already train %dK batches'% (len(record_df)))\n",
    "print('we have train: ', len(record_df), ' total epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auAdGn89yRTt"
   },
   "source": [
    "**Setting training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1622982464455,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "lmVPMoO-NK4K"
   },
   "outputs": [],
   "source": [
    "total_epochs = len(record_df)\n",
    "r_epochs = len(record_df)\n",
    "\n",
    "epoch_length = 1000\n",
    "num_epochs = 10\n",
    "iter_num = 0\n",
    "\n",
    "total_epochs += num_epochs\n",
    "\n",
    "losses = np.zeros((epoch_length, 5))\n",
    "rpn_accuracy_rpn_monitor = []\n",
    "rpn_accuracy_for_epoch = []\n",
    "\n",
    "if len(record_df)==0:\n",
    "    best_loss = np.Inf\n",
    "else:\n",
    "    best_loss = np.min(r_curr_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1206,
     "status": "ok",
     "timestamp": 1622982465652,
     "user": {
      "displayName": "Huy Nguyễn Minh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgG0pwFJtl-SJVu4m10Gk5ARyu1PSzDsad4Ky7N=s64",
      "userId": "11042980068344160235"
     },
     "user_tz": -420
    },
    "id": "Or1yhJ7e7z3c",
    "outputId": "4d5d54ae-e19b-4f7d-8251-6ba4e11e51ce"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.arange(0, r_epochs), record_df['mean_overlapping_bboxes'], 'r')\n",
    "plt.title('mean_overlapping_bboxes')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.arange(0, r_epochs), record_df['class_acc'], 'r')\n",
    "plt.title('class_acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.arange(0, r_epochs), record_df['loss_rpn_cls'], 'r')\n",
    "plt.title('loss_rpn_cls')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.arange(0, r_epochs), record_df['loss_rpn_regr'], 'r')\n",
    "plt.title('loss_rpn_regr')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.arange(0, r_epochs), record_df['loss_class_cls'], 'r')\n",
    "plt.title('loss_class_cls')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.arange(0, r_epochs), record_df['loss_class_regr'], 'r')\n",
    "plt.title('loss_class_regr')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(0, r_epochs), record_df['curr_loss'], 'r')\n",
    "plt.title('total_loss')\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(15,5))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.plot(np.arange(0, r_epochs), record_df['curr_loss'], 'r')\n",
    "# plt.title('total_loss')\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.plot(np.arange(0, r_epochs), record_df['elapsed_time'], 'r')\n",
    "# plt.title('elapsed_time')\n",
    "# plt.show()\n",
    "\n",
    "# plt.title('loss')\n",
    "# plt.plot(np.arange(0, r_epochs), record_df['loss_rpn_cls'], 'b')\n",
    "# plt.plot(np.arange(0, r_epochs), record_df['loss_rpn_regr'], 'g')\n",
    "# plt.plot(np.arange(0, r_epochs), record_df['loss_class_cls'], 'r')\n",
    "# plt.plot(np.arange(0, r_epochs), record_df['loss_class_regr'], 'c')\n",
    "# # plt.plot(np.arange(0, r_epochs), record_df['curr_loss'], 'm')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DYD8QniRx-b9",
    "outputId": "106cf8be-5e20-4682-d6da-67d785f85bc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/57\n",
      "   4/1000 [..............................] - ETA: 1:43:46 - rpn_cls: 4.9268 - rpn_regr: 0.0725 - final_cls: 0.2134 - final_regr: 0.0621WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "   5/1000 [..............................] - ETA: 1:40:58 - rpn_cls: 4.7908 - rpn_regr: 0.0709 - final_cls: 0.2133 - final_regr: 0.0638WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "   6/1000 [..............................] - ETA: 1:31:46 - rpn_cls: 4.6469 - rpn_regr: 0.0715 - final_cls: 0.2093 - final_regr: 0.0629WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "   7/1000 [..............................] - ETA: 1:27:32 - rpn_cls: 4.5097 - rpn_regr: 0.0853 - final_cls: 0.2100 - final_regr: 0.0622WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8/1000 [..............................] - ETA: 1:20:53 - rpn_cls: 4.3504 - rpn_regr: 0.0959 - final_cls: 0.2072 - final_regr: 0.0608WARNING:tensorflow:10 out of the last 10 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "  11/1000 [..............................] - ETA: 1:02:59 - rpn_cls: 3.8902 - rpn_regr: 0.1133 - final_cls: 0.2021 - final_regr: 0.0609WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "  12/1000 [..............................] - ETA: 1:01:46 - rpn_cls: 3.7592 - rpn_regr: 0.1177 - final_cls: 0.2039 - final_regr: 0.0634WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "  14/1000 [..............................] - ETA: 56:28 - rpn_cls: 3.5634 - rpn_regr: 0.1233 - final_cls: 0.2049 - final_regr: 0.0675 WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "  15/1000 [..............................] - ETA: 55:45 - rpn_cls: 3.4956 - rpn_regr: 0.1266 - final_cls: 0.2038 - final_regr: 0.0685WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "  18/1000 [..............................] - ETA: 50:20 - rpn_cls: 3.2885 - rpn_regr: 0.1312 - final_cls: 0.2026 - final_regr: 0.0704WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "  19/1000 [..............................] - ETA: 50:00 - rpn_cls: 3.2214 - rpn_regr: 0.1334 - final_cls: 0.2039 - final_regr: 0.0710WARNING:tensorflow:7 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "  22/1000 [..............................] - ETA: 45:11 - rpn_cls: 3.0440 - rpn_regr: 0.1371 - final_cls: 0.2042 - final_regr: 0.0723WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "  35/1000 [>.............................] - ETA: 35:16 - rpn_cls: 2.5890 - rpn_regr: 0.1425 - final_cls: 0.1982 - final_regr: 0.0807WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  37/1000 [>.............................] - ETA: 34:55 - rpn_cls: 2.5378 - rpn_regr: 0.1429 - final_cls: 0.1981 - final_regr: 0.0822WARNING:tensorflow:6 out of the last 16 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 16 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 16 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "  40/1000 [>.............................] - ETA: 33:26 - rpn_cls: 2.4657 - rpn_regr: 0.1446 - final_cls: 0.1987 - final_regr: 0.0852WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "  41/1000 [>.............................] - ETA: 33:42 - rpn_cls: 2.4427 - rpn_regr: 0.1453 - final_cls: 0.1995 - final_regr: 0.0862WARNING:tensorflow:7 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "  42/1000 [>.............................] - ETA: 34:10 - rpn_cls: 2.4207 - rpn_regr: 0.1461 - final_cls: 0.2003 - final_regr: 0.0873WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "  44/1000 [>.............................] - ETA: 33:31 - rpn_cls: 2.3773 - rpn_regr: 0.1473 - final_cls: 0.2016 - final_regr: 0.0892WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "  45/1000 [>.............................] - ETA: 34:00 - rpn_cls: 2.3560 - rpn_regr: 0.1479 - final_cls: 0.2024 - final_regr: 0.0900WARNING:tensorflow:7 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "  46/1000 [>.............................] - ETA: 34:18 - rpn_cls: 2.3364 - rpn_regr: 0.1485 - final_cls: 0.2031 - final_regr: 0.0908WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "  48/1000 [>.............................] - ETA: 33:48 - rpn_cls: 2.2978 - rpn_regr: 0.1495 - final_cls: 0.2045 - final_regr: 0.0922WARNING:tensorflow:8 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "  54/1000 [>.............................] - ETA: 31:24 - rpn_cls: 2.1871 - rpn_regr: 0.1527 - final_cls: 0.2113 - final_regr: 0.0964WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  63/1000 [>.............................] - ETA: 29:40 - rpn_cls: 2.0504 - rpn_regr: 0.1568 - final_cls: 0.2164 - final_regr: 0.1012WARNING:tensorflow:5 out of the last 16 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "  67/1000 [=>............................] - ETA: 29:10 - rpn_cls: 2.0039 - rpn_regr: 0.1581 - final_cls: 0.2175 - final_regr: 0.1030WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "  68/1000 [=>............................] - ETA: 29:21 - rpn_cls: 1.9925 - rpn_regr: 0.1583 - final_cls: 0.2177 - final_regr: 0.1034WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "  71/1000 [=>............................] - ETA: 28:42 - rpn_cls: 1.9588 - rpn_regr: 0.1589 - final_cls: 0.2180 - final_regr: 0.1044WARNING:tensorflow:6 out of the last 14 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "  82/1000 [=>............................] - ETA: 27:38 - rpn_cls: 1.8443 - rpn_regr: 0.1603 - final_cls: 0.2203 - final_regr: 0.1071WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "  84/1000 [=>............................] - ETA: 27:33 - rpn_cls: 1.8264 - rpn_regr: 0.1604 - final_cls: 0.2210 - final_regr: 0.1075WARNING:tensorflow:6 out of the last 16 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 16 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 137/1000 [===>..........................] - ETA: 20:56 - rpn_cls: 1.5127 - rpn_regr: 0.1601 - final_cls: 0.2360 - final_regr: 0.1185WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 139/1000 [===>..........................] - ETA: 20:51 - rpn_cls: 1.5044 - rpn_regr: 0.1601 - final_cls: 0.2364 - final_regr: 0.1188WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 156/1000 [===>..........................] - ETA: 19:39 - rpn_cls: 1.4401 - rpn_regr: 0.1606 - final_cls: 0.2399 - final_regr: 0.1218WARNING:tensorflow:5 out of the last 18 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 18 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 18 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 160/1000 [===>..........................] - ETA: 19:29 - rpn_cls: 1.4268 - rpn_regr: 0.1608 - final_cls: 0.2406 - final_regr: 0.1224WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 22 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 161/1000 [===>..........................] - ETA: 19:38 - rpn_cls: 1.4234 - rpn_regr: 0.1608 - final_cls: 0.2408 - final_regr: 0.1226WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 163/1000 [===>..........................] - ETA: 19:38 - rpn_cls: 1.4169 - rpn_regr: 0.1609 - final_cls: 0.2411 - final_regr: 0.1230WARNING:tensorflow:6 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 164/1000 [===>..........................] - ETA: 19:45 - rpn_cls: 1.4137 - rpn_regr: 0.1610 - final_cls: 0.2413 - final_regr: 0.1232WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 165/1000 [===>..........................] - ETA: 19:51 - rpn_cls: 1.4105 - rpn_regr: 0.1611 - final_cls: 0.2415 - final_regr: 0.1234WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 166/1000 [===>..........................] - ETA: 19:58 - rpn_cls: 1.4074 - rpn_regr: 0.1611 - final_cls: 0.2417 - final_regr: 0.1236WARNING:tensorflow:7 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 169/1000 [====>.........................] - ETA: 19:53 - rpn_cls: 1.3983 - rpn_regr: 0.1613 - final_cls: 0.2423 - final_regr: 0.1241WARNING:tensorflow:8 out of the last 15 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 14 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 173/1000 [====>.........................] - ETA: 19:44 - rpn_cls: 1.3865 - rpn_regr: 0.1614 - final_cls: 0.2431 - final_regr: 0.1247WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 197/1000 [====>.........................] - ETA: 18:39 - rpn_cls: 1.3239 - rpn_regr: 0.1633 - final_cls: 0.2475 - final_regr: 0.1283WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 214/1000 [=====>........................] - ETA: 18:04 - rpn_cls: 1.2874 - rpn_regr: 0.1653 - final_cls: 0.2500 - final_regr: 0.1308WARNING:tensorflow:5 out of the last 15 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 217/1000 [=====>........................] - ETA: 17:59 - rpn_cls: 1.2813 - rpn_regr: 0.1656 - final_cls: 0.2505 - final_regr: 0.1312WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 241/1000 [======>.......................] - ETA: 16:53 - rpn_cls: 1.2395 - rpn_regr: 0.1675 - final_cls: 0.2537 - final_regr: 0.1339WARNING:tensorflow:5 out of the last 21 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 21 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 19 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 243/1000 [======>.......................] - ETA: 16:54 - rpn_cls: 1.2363 - rpn_regr: 0.1676 - final_cls: 0.2539 - final_regr: 0.1341WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 21 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 307/1000 [========>.....................] - ETA: 14:21 - rpn_cls: 1.1554 - rpn_regr: 0.1701 - final_cls: 0.2591 - final_regr: 0.1385WARNING:tensorflow:5 out of the last 18 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 322/1000 [========>.....................] - ETA: 13:55 - rpn_cls: 1.1404 - rpn_regr: 0.1704 - final_cls: 0.2602 - final_regr: 0.1392WARNING:tensorflow:5 out of the last 16 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 16 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 16 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 324/1000 [========>.....................] - ETA: 13:56 - rpn_cls: 1.1384 - rpn_regr: 0.1704 - final_cls: 0.2604 - final_regr: 0.1393WARNING:tensorflow:6 out of the last 18 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 18 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 18 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 325/1000 [========>.....................] - ETA: 13:58 - rpn_cls: 1.1374 - rpn_regr: 0.1705 - final_cls: 0.2604 - final_regr: 0.1394WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 326/1000 [========>.....................] - ETA: 13:59 - rpn_cls: 1.1364 - rpn_regr: 0.1705 - final_cls: 0.2605 - final_regr: 0.1394WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 373/1000 [==========>...................] - ETA: 12:32 - rpn_cls: 1.0946 - rpn_regr: 0.1716 - final_cls: 0.2630 - final_regr: 0.1416WARNING:tensorflow:5 out of the last 24 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 374/1000 [==========>...................] - ETA: 12:33 - rpn_cls: 1.0938 - rpn_regr: 0.1716 - final_cls: 0.2630 - final_regr: 0.1416WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 25 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 377/1000 [==========>...................] - ETA: 12:29 - rpn_cls: 1.0916 - rpn_regr: 0.1717 - final_cls: 0.2631 - final_regr: 0.1417WARNING:tensorflow:6 out of the last 15 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 380/1000 [==========>...................] - ETA: 12:27 - rpn_cls: 1.0893 - rpn_regr: 0.1717 - final_cls: 0.2632 - final_regr: 0.1417WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 560/1000 [===============>..............] - ETA: 8:11 - rpn_cls: 0.9869 - rpn_regr: 0.1750 - final_cls: 0.2649 - final_regr: 0.1442WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 561/1000 [===============>..............] - ETA: 8:11 - rpn_cls: 0.9866 - rpn_regr: 0.1750 - final_cls: 0.2649 - final_regr: 0.1442WARNING:tensorflow:6 out of the last 14 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 566/1000 [===============>..............] - ETA: 8:04 - rpn_cls: 0.9847 - rpn_regr: 0.1751 - final_cls: 0.2649 - final_regr: 0.1443WARNING:tensorflow:6 out of the last 15 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 14 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 647/1000 [==================>...........] - ETA: 6:24 - rpn_cls: 0.9571 - rpn_regr: 0.1755 - final_cls: 0.2650 - final_regr: 0.1450WARNING:tensorflow:5 out of the last 18 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 18 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 649/1000 [==================>...........] - ETA: 6:22 - rpn_cls: 0.9566 - rpn_regr: 0.1755 - final_cls: 0.2650 - final_regr: 0.1450WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c5c0f2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c7c0bf670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 19 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0c146d2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 960/1000 [===========================>..] - ETA: 39s - rpn_cls: 0.8877 - rpn_regr: 0.1770 - final_cls: 0.2637 - final_regr: 0.1483Average number of overlapping bounding boxes from RPN = 30.055 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 988s 988ms/step - rpn_cls: 0.8814 - rpn_regr: 0.1773 - final_cls: 0.2635 - final_regr: 0.1486\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 29.966507177033492\n",
      "Classifier accuracy for bounding boxes from RPN: 0.8905\n",
      "Loss RPN classifier: 0.7271801605857309\n",
      "Loss RPN regression: 0.18519080852740444\n",
      "Loss Detector classifier: 0.2606367125077276\n",
      "Loss Detector regression: 0.15602823667414487\n",
      "Total loss: 1.3290359182950078\n",
      "Elapsed time: 988.0440981388092\n",
      " 918/1000 [==========================>...] - ETA: 2:25 - rpn_cls: 0.7655 - rpn_regr: 0.1912 - final_cls: 0.2545 - final_regr: 0.1482Average number of overlapping bounding boxes from RPN = 29.676 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 1681s 2s/step - rpn_cls: 0.7599 - rpn_regr: 0.1911 - final_cls: 0.2546 - final_regr: 0.1486\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 30.170520231213874\n",
      "Classifier accuracy for bounding boxes from RPN: 0.892\n",
      "Loss RPN classifier: 0.6317614097782311\n",
      "Loss RPN regression: 0.19054361731442623\n",
      "Loss Detector classifier: 0.25420555464923583\n",
      "Loss Detector regression: 0.15676144849602133\n",
      "Total loss: 1.2332720302379145\n",
      "Elapsed time: 693.0106410980225\n",
      " 556/1000 [===============>..............] - ETA: 27:14 - rpn_cls: 0.7460 - rpn_regr: 0.1933 - final_cls: 0.2562 - final_regr: 0.1521Exception: in user code:\n",
      "\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n",
      "        outputs = self.distribute_strategy.run(\n",
      "    <ipython-input-29-5c5a1b9cddfe>:34 call  *\n",
      "        x = rois[0, roi_idx, 0]\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:973 _slice_helper\n",
      "        return strided_slice(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1140 strided_slice\n",
      "        op = gen_array_ops.strided_slice(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:10174 strided_slice\n",
      "        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:742 _apply_op_helper\n",
      "        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:593 _create_op_internal\n",
      "        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:3319 _create_op_internal\n",
      "        ret = Operation(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1816 __init__\n",
      "        self._c_op = _create_c_op(self._graph, node_def, inputs,\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1657 _create_c_op\n",
      "        raise ValueError(str(e))\n",
      "\n",
      "    ValueError: slice index 1 of dimension 1 out of bounds. for '{{node model_1/roi_pooling_conv/strided_slice_5}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=7](model_1/Cast, model_1/roi_pooling_conv/strided_slice_5/stack, model_1/roi_pooling_conv/strided_slice_5/stack_1, model_1/roi_pooling_conv/strided_slice_5/stack_2)' with input shapes: [1,1,4], [3], [3], [3] and with computed input tensors: input[1] = <0 1 0>, input[2] = <1 2 1>, input[3] = <1 1 1>.\n",
      "\n",
      "Epoch 49/57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 879/1000 [=========================>....] - ETA: 29s - rpn_cls: 0.6478 - rpn_regr: 0.1997 - final_cls: 0.2686 - final_regr: 0.1502Average number of overlapping bounding boxes from RPN = 30.192 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 288s 288ms/step - rpn_cls: 0.6487 - rpn_regr: 0.1989 - final_cls: 0.2677 - final_regr: 0.1500\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 29.770334928229666\n",
      "Classifier accuracy for bounding boxes from RPN: 0.896\n",
      "Loss RPN classifier: 0.647942482911205\n",
      "Loss RPN regression: 0.1904307258722838\n",
      "Loss Detector classifier: 0.264102852404177\n",
      "Loss Detector regression: 0.14962953800987452\n",
      "Total loss: 1.2521055991975403\n",
      "Elapsed time: 654.6273720264435\n",
      "  44/1000 [>.............................] - ETA: 1:55:49 - rpn_cls: 0.6611 - rpn_regr: 0.1995 - final_cls: 0.2685 - final_regr: 0.1501Exception: in user code:\n",
      "\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n",
      "        outputs = self.distribute_strategy.run(\n",
      "    <ipython-input-29-5c5a1b9cddfe>:34 call  *\n",
      "        x = rois[0, roi_idx, 0]\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:973 _slice_helper\n",
      "        return strided_slice(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1140 strided_slice\n",
      "        op = gen_array_ops.strided_slice(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:10174 strided_slice\n",
      "        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:742 _apply_op_helper\n",
      "        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:593 _create_op_internal\n",
      "        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:3319 _create_op_internal\n",
      "        ret = Operation(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1816 __init__\n",
      "        self._c_op = _create_c_op(self._graph, node_def, inputs,\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1657 _create_c_op\n",
      "        raise ValueError(str(e))\n",
      "\n",
      "    ValueError: slice index 2 of dimension 1 out of bounds. for '{{node model_1/roi_pooling_conv/strided_slice_10}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=7](model_1/Cast, model_1/roi_pooling_conv/strided_slice_10/stack, model_1/roi_pooling_conv/strided_slice_10/stack_1, model_1/roi_pooling_conv/strided_slice_10/stack_2)' with input shapes: [1,2,4], [3], [3], [3] and with computed input tensors: input[1] = <0 2 0>, input[2] = <1 3 1>, input[3] = <1 1 1>.\n",
      "\n",
      "Epoch 50/57\n",
      " 642/1000 [==================>...........] - ETA: 3:28 - rpn_cls: 0.6747 - rpn_regr: 0.2109 - final_cls: 0.2501 - final_regr: 0.1587Exception: in user code:\n",
      "\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n",
      "        outputs = self.distribute_strategy.run(\n",
      "    <ipython-input-29-5c5a1b9cddfe>:34 call  *\n",
      "        x = rois[0, roi_idx, 0]\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:973 _slice_helper\n",
      "        return strided_slice(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1140 strided_slice\n",
      "        op = gen_array_ops.strided_slice(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:10174 strided_slice\n",
      "        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:742 _apply_op_helper\n",
      "        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:593 _create_op_internal\n",
      "        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:3319 _create_op_internal\n",
      "        ret = Operation(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1816 __init__\n",
      "        self._c_op = _create_c_op(self._graph, node_def, inputs,\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1657 _create_c_op\n",
      "        raise ValueError(str(e))\n",
      "\n",
      "    ValueError: slice index 1 of dimension 1 out of bounds. for '{{node model_1/roi_pooling_conv/strided_slice_5}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=7](model_1/Cast, model_1/roi_pooling_conv/strided_slice_5/stack, model_1/roi_pooling_conv/strided_slice_5/stack_1, model_1/roi_pooling_conv/strided_slice_5/stack_2)' with input shapes: [1,1,4], [3], [3], [3] and with computed input tensors: input[1] = <0 1 0>, input[2] = <1 2 1>, input[3] = <1 1 1>.\n",
      "\n",
      "Epoch 51/57\n",
      " 830/1000 [=======================>......] - ETA: 23s - rpn_cls: 0.6195 - rpn_regr: 0.2012 - final_cls: 0.2493 - final_regr: 0.1557Average number of overlapping bounding boxes from RPN = 30.012 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 220s 220ms/step - rpn_cls: 0.6223 - rpn_regr: 0.2002 - final_cls: 0.2494 - final_regr: 0.1559\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 30.02\n",
      "Classifier accuracy for bounding boxes from RPN: 0.8945\n",
      "Loss RPN classifier: 0.6416868199523844\n",
      "Loss RPN regression: 0.1938965524369851\n",
      "Loss Detector classifier: 0.25142386921704174\n",
      "Loss Detector regression: 0.15803817011835053\n",
      "Total loss: 1.2450454117247616\n",
      "Elapsed time: 626.0391294956207\n",
      " 783/1000 [======================>.......] - ETA: 3:13 - rpn_cls: 0.6248 - rpn_regr: 0.1882 - final_cls: 0.2619 - final_regr: 0.1579Average number of overlapping bounding boxes from RPN = 30.291 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 833s 833ms/step - rpn_cls: 0.6220 - rpn_regr: 0.1880 - final_cls: 0.2614 - final_regr: 0.1573\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 29.85238095238095\n",
      "Classifier accuracy for bounding boxes from RPN: 0.894\n",
      "Loss RPN classifier: 0.5997041009208505\n",
      "Loss RPN regression: 0.1862797347594751\n",
      "Loss Detector classifier: 0.2564799781590084\n",
      "Loss Detector regression: 0.1522348769651726\n",
      "Total loss: 1.1946986908045067\n",
      "Elapsed time: 613.185129404068\n",
      " 737/1000 [=====================>........] - ETA: 7:33 - rpn_cls: 0.6240 - rpn_regr: 0.1856 - final_cls: 0.2633 - final_regr: 0.1570Average number of overlapping bounding boxes from RPN = 29.934 for 1000 previous iterations\n",
      " 814/1000 [=======================>......] - ETA: 4:59 - rpn_cls: 0.6228 - rpn_regr: 0.1857 - final_cls: 0.2627 - final_regr: 0.1568Exception: in user code:\n",
      "\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n",
      "        outputs = self.distribute_strategy.run(\n",
      "    <ipython-input-29-5c5a1b9cddfe>:34 call  *\n",
      "        x = rois[0, roi_idx, 0]\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:973 _slice_helper\n",
      "        return strided_slice(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1140 strided_slice\n",
      "        op = gen_array_ops.strided_slice(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:10174 strided_slice\n",
      "        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:742 _apply_op_helper\n",
      "        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:593 _create_op_internal\n",
      "        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:3319 _create_op_internal\n",
      "        ret = Operation(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1816 __init__\n",
      "        self._c_op = _create_c_op(self._graph, node_def, inputs,\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1657 _create_c_op\n",
      "        raise ValueError(str(e))\n",
      "\n",
      "    ValueError: slice index 1 of dimension 1 out of bounds. for '{{node model_1/roi_pooling_conv/strided_slice_5}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=7](model_1/Cast, model_1/roi_pooling_conv/strided_slice_5/stack, model_1/roi_pooling_conv/strided_slice_5/stack_1, model_1/roi_pooling_conv/strided_slice_5/stack_2)' with input shapes: [1,1,4], [3], [3], [3] and with computed input tensors: input[1] = <0 1 0>, input[2] = <1 2 1>, input[3] = <1 1 1>.\n",
      "\n",
      "Epoch 52/57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 112s 112ms/step - rpn_cls: 0.5804 - rpn_regr: 0.1889 - final_cls: 0.2428 - final_regr: 0.1503\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 29.77151051625239\n",
      "Classifier accuracy for bounding boxes from RPN: 0.90175\n",
      "Loss RPN classifier: 0.5855305782325705\n",
      "Loss RPN regression: 0.18686295066494496\n",
      "Loss Detector classifier: 0.24004126516133448\n",
      "Loss Detector regression: 0.14838262210600078\n",
      "Total loss: 1.1608174161648506\n",
      "Elapsed time: 591.5128877162933\n",
      "Total loss decreased from 1.1909999999999998 to 1.1608174161648506, saving weights\n",
      "Epoch 53/57\n",
      " 535/1000 [===============>..............] - ETA: 4:37 - rpn_cls: 0.7190 - rpn_regr: 0.1698 - final_cls: 0.2716 - final_regr: 0.1565Exception: in user code:\n",
      "\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n",
      "        outputs = self.distribute_strategy.run(\n",
      "    <ipython-input-29-5c5a1b9cddfe>:34 call  *\n",
      "        x = rois[0, roi_idx, 0]\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:973 _slice_helper\n",
      "        return strided_slice(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1140 strided_slice\n",
      "        op = gen_array_ops.strided_slice(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:10174 strided_slice\n",
      "        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:742 _apply_op_helper\n",
      "        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:593 _create_op_internal\n",
      "        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:3319 _create_op_internal\n",
      "        ret = Operation(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1816 __init__\n",
      "        self._c_op = _create_c_op(self._graph, node_def, inputs,\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1657 _create_c_op\n",
      "        raise ValueError(str(e))\n",
      "\n",
      "    ValueError: slice index 1 of dimension 1 out of bounds. for '{{node model_1/roi_pooling_conv/strided_slice_5}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=7](model_1/Cast, model_1/roi_pooling_conv/strided_slice_5/stack, model_1/roi_pooling_conv/strided_slice_5/stack_1, model_1/roi_pooling_conv/strided_slice_5/stack_2)' with input shapes: [1,1,4], [3], [3], [3] and with computed input tensors: input[1] = <0 1 0>, input[2] = <1 2 1>, input[3] = <1 1 1>.\n",
      "\n",
      "Epoch 54/57\n",
      " 689/1000 [===================>..........] - ETA: 39s - rpn_cls: 0.6745 - rpn_regr: 0.1899 - final_cls: 0.2553 - final_regr: 0.1603Average number of overlapping bounding boxes from RPN = 30.163 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 271s 271ms/step - rpn_cls: 0.6697 - rpn_regr: 0.1890 - final_cls: 0.2545 - final_regr: 0.1585\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 30.334600760456272\n",
      "Classifier accuracy for bounding boxes from RPN: 0.8965\n",
      "Loss RPN classifier: 0.6488753169384783\n",
      "Loss RPN regression: 0.18685275495564566\n",
      "Loss Detector classifier: 0.2516484054910705\n",
      "Loss Detector regression: 0.15370670909201725\n",
      "Total loss: 1.2410831864772116\n",
      "Elapsed time: 596.8794836997986\n",
      " 640/1000 [==================>...........] - ETA: 6:05 - rpn_cls: 0.6108 - rpn_regr: 0.1817 - final_cls: 0.2451 - final_regr: 0.1531Average number of overlapping bounding boxes from RPN = 29.118 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 859s 859ms/step - rpn_cls: 0.6083 - rpn_regr: 0.1821 - final_cls: 0.2457 - final_regr: 0.1528\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 29.759505703422054\n",
      "Classifier accuracy for bounding boxes from RPN: 0.89775\n",
      "Loss RPN classifier: 0.5969007497382434\n",
      "Loss RPN regression: 0.1841756584502291\n",
      "Loss Detector classifier: 0.2537599151470146\n",
      "Loss Detector regression: 0.1473114348128438\n",
      "Total loss: 1.1821477581483308\n",
      "Elapsed time: 587.8027522563934\n",
      " 589/1000 [================>.............] - ETA: 14:04 - rpn_cls: 0.5967 - rpn_regr: 0.1804 - final_cls: 0.2524 - final_regr: 0.1516Average number of overlapping bounding boxes from RPN = 29.94 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 1450s 1s/step - rpn_cls: 0.5961 - rpn_regr: 0.1803 - final_cls: 0.2546 - final_regr: 0.1509\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 30.45385347288297\n",
      "Classifier accuracy for bounding boxes from RPN: 0.887\n",
      "Loss RPN classifier: 0.5783778878741396\n",
      "Loss RPN regression: 0.17902456891618204\n",
      "Loss Detector classifier: 0.2703325891995578\n",
      "Loss Detector regression: 0.14761666481755673\n",
      "Total loss: 1.175351710807436\n",
      "Elapsed time: 590.950868844986\n",
      " 550/1000 [===============>..............] - ETA: 24:12 - rpn_cls: 0.6030 - rpn_regr: 0.1807 - final_cls: 0.2531 - final_regr: 0.1504Average number of overlapping bounding boxes from RPN = 30.156 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 2040s 2s/step - rpn_cls: 0.6040 - rpn_regr: 0.1807 - final_cls: 0.2521 - final_regr: 0.1501\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 29.86\n",
      "Classifier accuracy for bounding boxes from RPN: 0.89775\n",
      "Loss RPN classifier: 0.6075427879381107\n",
      "Loss RPN regression: 0.18583823822202977\n",
      "Loss Detector classifier: 0.2472538034589538\n",
      "Loss Detector regression: 0.1479745617713779\n",
      "Total loss: 1.188609391390472\n",
      "Elapsed time: 589.7446620464325\n",
      " 499/1000 [=============>................] - ETA: 39:04 - rpn_cls: 0.5939 - rpn_regr: 0.1797 - final_cls: 0.2520 - final_regr: 0.1519Average number of overlapping bounding boxes from RPN = 29.747 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 2628s 3s/step - rpn_cls: 0.5927 - rpn_regr: 0.1792 - final_cls: 0.2517 - final_regr: 0.1523\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 30.101904761904763\n",
      "Classifier accuracy for bounding boxes from RPN: 0.89275\n",
      "Loss RPN classifier: 0.5757572228806025\n",
      "Loss RPN regression: 0.1777523875342449\n",
      "Loss Detector classifier: 0.2495935966729933\n",
      "Loss Detector regression: 0.150511549948249\n",
      "Total loss: 1.1536147570360897\n",
      "Elapsed time: 588.4761776924133\n",
      "Total loss decreased from 1.1608174161648506 to 1.1536147570360897, saving weights\n",
      "Epoch 55/57\n",
      " 448/1000 [============>.................] - ETA: 5:31 - rpn_cls: 0.6684 - rpn_regr: 0.1858 - final_cls: 0.2306 - final_regr: 0.1420Average number of overlapping bounding boxes from RPN = 29.895 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 598s 598ms/step - rpn_cls: 0.6305 - rpn_regr: 0.1796 - final_cls: 0.2456 - final_regr: 0.1420\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 29.21585482330468\n",
      "Classifier accuracy for bounding boxes from RPN: 0.90225\n",
      "Loss RPN classifier: 0.6193759913361608\n",
      "Loss RPN regression: 0.18016155772353523\n",
      "Loss Detector classifier: 0.2525247915901868\n",
      "Loss Detector regression: 0.14053576605580748\n",
      "Total loss: 1.1925981067056903\n",
      "Elapsed time: 605.797661781311\n",
      " 407/1000 [===========>..................] - ETA: 20:16 - rpn_cls: 0.6151 - rpn_regr: 0.1752 - final_cls: 0.2292 - final_regr: 0.1383Average number of overlapping bounding boxes from RPN = 30.534 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 1190s 1s/step - rpn_cls: 0.6169 - rpn_regr: 0.1746 - final_cls: 0.2235 - final_regr: 0.1381\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 29.945714285714285\n",
      "Classifier accuracy for bounding boxes from RPN: 0.9085\n",
      "Loss RPN classifier: 0.6178972338104566\n",
      "Loss RPN regression: 0.17705366006644907\n",
      "Loss Detector classifier: 0.22560209136459775\n",
      "Loss Detector regression: 0.1406831917287782\n",
      "Total loss: 1.1612361769702817\n",
      "Elapsed time: 591.6257452964783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 355/1000 [=========>....................] - ETA: 42:16 - rpn_cls: 0.6127 - rpn_regr: 0.1794 - final_cls: 0.2238 - final_regr: 0.1368Average number of overlapping bounding boxes from RPN = 29.846 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 1788s 2s/step - rpn_cls: 0.6222 - rpn_regr: 0.1803 - final_cls: 0.2274 - final_regr: 0.1386\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 29.418957345971563\n",
      "Classifier accuracy for bounding boxes from RPN: 0.899\n",
      "Loss RPN classifier: 0.6299257103628371\n",
      "Loss RPN regression: 0.18107422014288022\n",
      "Loss Detector classifier: 0.24128157826285154\n",
      "Loss Detector regression: 0.1442442237669602\n",
      "Total loss: 1.196525732535529\n",
      "Elapsed time: 598.0818469524384\n",
      " 304/1000 [========>.....................] - ETA: 1:14:56 - rpn_cls: 0.6220 - rpn_regr: 0.1842 - final_cls: 0.2300 - final_regr: 0.1380Average number of overlapping bounding boxes from RPN = 29.119 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 2372s 2s/step - rpn_cls: 0.6176 - rpn_regr: 0.1852 - final_cls: 0.2336 - final_regr: 0.1379\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 29.590996168582375\n",
      "Classifier accuracy for bounding boxes from RPN: 0.8945\n",
      "Loss RPN classifier: 0.5987510042771859\n",
      "Loss RPN regression: 0.1839570599880535\n",
      "Loss Detector classifier: 0.250462801568194\n",
      "Loss Detector regression: 0.13928503556037322\n",
      "Total loss: 1.1724559013938065\n",
      "Elapsed time: 584.1540615558624\n",
      " 261/1000 [======>.......................] - ETA: 1:59:06 - rpn_cls: 0.6118 - rpn_regr: 0.1847 - final_cls: 0.2330 - final_regr: 0.1394Average number of overlapping bounding boxes from RPN = 29.564 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 2960s 3s/step - rpn_cls: 0.6114 - rpn_regr: 0.1843 - final_cls: 0.2322 - final_regr: 0.1398\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 29.34165067178503\n",
      "Classifier accuracy for bounding boxes from RPN: 0.90375\n",
      "Loss RPN classifier: 0.6026362832168664\n",
      "Loss RPN regression: 0.17563739041471854\n",
      "Loss Detector classifier: 0.24276501460990824\n",
      "Loss Detector regression: 0.14433440794609487\n",
      "Total loss: 1.165373096187588\n",
      "Elapsed time: 588.500862121582\n",
      " 219/1000 [=====>........................] - ETA: 3:03:40 - rpn_cls: 0.6110 - rpn_regr: 0.1842 - final_cls: 0.2351 - final_regr: 0.1404Average number of overlapping bounding boxes from RPN = 29.44 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 3553s 4s/step - rpn_cls: 0.6033 - rpn_regr: 0.1845 - final_cls: 0.2383 - final_regr: 0.1410\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 29.51189343482398\n",
      "Classifier accuracy for bounding boxes from RPN: 0.90275\n",
      "Loss RPN classifier: 0.563405133256329\n",
      "Loss RPN regression: 0.17944341618556064\n",
      "Loss Detector classifier: 0.2482171826387745\n",
      "Loss Detector regression: 0.14571263058390468\n",
      "Total loss: 1.1367783626645689\n",
      "Elapsed time: 592.2014586925507\n",
      "Total loss decreased from 1.1536147570360897 to 1.1367783626645689, saving weights\n",
      "Epoch 56/57\n",
      " 174/1000 [====>.........................] - ETA: 8:02 - rpn_cls: 0.6419 - rpn_regr: 0.1740 - final_cls: 0.2061 - final_regr: 0.1495Average number of overlapping bounding boxes from RPN = 29.451 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 586s 586ms/step - rpn_cls: 0.5882 - rpn_regr: 0.1863 - final_cls: 0.2370 - final_regr: 0.1491\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 29.61376673040153\n",
      "Classifier accuracy for bounding boxes from RPN: 0.89975\n",
      "Loss RPN classifier: 0.5768909564449898\n",
      "Loss RPN regression: 0.1840796064469032\n",
      "Loss Detector classifier: 0.24163494042894237\n",
      "Loss Detector regression: 0.14154216112196447\n",
      "Total loss: 1.1441476644428\n",
      "Elapsed time: 592.8270614147186\n",
      " 132/1000 [==>...........................] - ETA: 1:12:52 - rpn_cls: 0.5771 - rpn_regr: 0.1862 - final_cls: 0.2400 - final_regr: 0.1472Average number of overlapping bounding boxes from RPN = 29.332 for 1000 previous iterations\n",
      " 205/1000 [=====>........................] - ETA: 45:44 - rpn_cls: 0.5660 - rpn_regr: 0.1850 - final_cls: 0.2390 - final_regr: 0.1464Exception: in user code:\n",
      "\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n",
      "        outputs = self.distribute_strategy.run(\n",
      "    <ipython-input-29-5c5a1b9cddfe>:34 call  *\n",
      "        x = rois[0, roi_idx, 0]\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:973 _slice_helper\n",
      "        return strided_slice(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1140 strided_slice\n",
      "        op = gen_array_ops.strided_slice(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:10174 strided_slice\n",
      "        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:742 _apply_op_helper\n",
      "        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:593 _create_op_internal\n",
      "        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:3319 _create_op_internal\n",
      "        ret = Operation(\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1816 __init__\n",
      "        self._c_op = _create_c_op(self._graph, node_def, inputs,\n",
      "    /home/hnm942/anaconda3/envs/fast-rcnn/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1657 _create_c_op\n",
      "        raise ValueError(str(e))\n",
      "\n",
      "    ValueError: slice index 1 of dimension 1 out of bounds. for '{{node model_1/roi_pooling_conv/strided_slice_5}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=7](model_1/Cast, model_1/roi_pooling_conv/strided_slice_5/stack, model_1/roi_pooling_conv/strided_slice_5/stack_1, model_1/roi_pooling_conv/strided_slice_5/stack_2)' with input shapes: [1,1,4], [3], [3], [3] and with computed input tensors: input[1] = <0 1 0>, input[2] = <1 2 1>, input[3] = <1 1 1>.\n",
      "\n",
      "Epoch 57/57\n",
      "1000/1000 [==============================] - 470s 470ms/step - rpn_cls: 0.4527 - rpn_regr: 0.1764 - final_cls: 0.2369 - final_regr: 0.1323\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 30.086584205518555\n",
      "Classifier accuracy for bounding boxes from RPN: 0.90225\n",
      "Loss RPN classifier: 0.5262771239956385\n",
      "Loss RPN regression: 0.17951281958224719\n",
      "Loss Detector classifier: 0.2365043833510541\n",
      "Loss Detector regression: 0.1320413926220499\n",
      "Total loss: 1.0743357195509897\n",
      "Elapsed time: 592.5675661563873\n",
      "Total loss decreased from 1.1367783626645689 to 1.0743357195509897, saving weights\n",
      "Training complete, exiting.\n"
     ]
    }
   ],
   "source": [
    "tf.function(experimental_relax_shapes=True)\n",
    "start_time = time.time()\n",
    "for epoch_num in range(num_epochs):\n",
    "  progbar = generic_utils.Progbar(epoch_length)\n",
    "  print('Epoch {}/{}'.format(r_epochs + 1, total_epochs))\n",
    "  r_epochs += 1\n",
    "  while True:\n",
    "    try:\n",
    "      if len(rpn_accuracy_rpn_monitor) == epoch_length and config.verbose:\n",
    "        mean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)\n",
    "        rpn_accuracy_rpn_monitor = []\n",
    "        print('Average number of overlapping bounding boxes from RPN = {} for {} previous iterations'.format(mean_overlapping_bboxes, epoch_length))\n",
    "        if mean_overlapping_bboxes == 0:\n",
    "          print('RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.')\n",
    "      # Generate X (x_img) and label Y ([y_rpn_cls, y_rpn_regr])\n",
    "      X, Y, img_data, debug_img, debug_num_pos = next(data_gen_train)\n",
    "      # Train rpn model and get loss value [loss_rpn_cls, loss_rpn_regr]\n",
    "      loss_rpn = model_rpn.train_on_batch(X, Y)\n",
    "      # Get predicted rpn from rpn model [rpn_cls, rpn_regr]\n",
    "      P_rpn = model_rpn.predict_on_batch(X)\n",
    "      bboxes = rpn_to_roi(P_rpn[0], P_rpn[1], config)\n",
    "      # note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format\n",
    "      # X2: bboxes that iou > config.classifier_min_overlap for all gt bboxes in 300 non_max_suppression bboxes\n",
    "      # Y1: one hot code for bboxes from above => x_roi (X)\n",
    "      # Y2: corresponding labels and corresponding gt bboxes\n",
    "      X2, Y1, Y2, IoUs = convert_coor(bboxes, img_data, config, class_mapping)\n",
    "      # If X2 is None means there are no matching bboxes\n",
    "      if X2 is None:\n",
    "        rpn_accuracy_rpn_monitor.append(0)\n",
    "        rpn_accuracy_for_epoch.append(0)\n",
    "        continue\n",
    "      # Find out the positive anchors and negative anchors\n",
    "      neg_samples = np.where(Y1[0, :, -1] == 1)\n",
    "      pos_samples = np.where(Y1[0, :, -1] == 0)\n",
    "      if len(neg_samples) > 0:\n",
    "        neg_samples = neg_samples[0]\n",
    "      else:\n",
    "        neg_samples = []\n",
    "      if len(pos_samples) > 0:\n",
    "        pos_samples = pos_samples[0]\n",
    "      else:\n",
    "        pos_samples = []\n",
    "      rpn_accuracy_rpn_monitor.append(len(pos_samples))\n",
    "      rpn_accuracy_for_epoch.append((len(pos_samples)))\n",
    "      if config.num_rois > 1:\n",
    "        # If number of positive anchors is larger than 4//2 = 2, randomly choose 2 pos samples\n",
    "        if len(pos_samples) < config.num_rois//2:\n",
    "          selected_pos_samples = pos_samples.tolist()\n",
    "        else:\n",
    "          selected_pos_samples = np.random.choice(pos_samples, config.num_rois//2, replace=False).tolist()\n",
    "         # Randomly choose (num_rois - num_pos) neg samples\n",
    "        try:\n",
    "          selected_neg_samples = np.random.choice(neg_samples, config.num_rois - len(selected_pos_samples), replace=False).tolist()\n",
    "        except:\n",
    "          selected_neg_samples = np.random.choice(neg_samples, config.num_rois - len(selected_pos_samples) if len(neg_samples)>0 else 0, replace=True).tolist()\n",
    "        # Save all the pos and neg samples in sel_samples\n",
    "        sel_samples = selected_pos_samples + selected_neg_samples\n",
    "      else:\n",
    "        # in the extreme case where num_rois = 1, we pick a random pos or neg sample\n",
    "        selected_pos_samples = pos_samples.tolist()\n",
    "        selected_neg_samples = neg_samples.tolist()\n",
    "        if np.random.randint(0, 2):\n",
    "          sel_samples = random.choice(neg_samples)\n",
    "        else:\n",
    "          sel_samples = random.choice(pos_samples)\n",
    "      # training_data: [X, X2[:, sel_samples, :]]  \n",
    "      # labels: [Y1[:, sel_samples, :], Y2[:, sel_samples, :]]\n",
    "      #  X                     => img_data resized image\n",
    "      #  X2[:, sel_samples, :] => num_rois (4 in here) bboxes which contains selected neg and pos\n",
    "      #  Y1[:, sel_samples, :] => one hot encode for num_rois bboxes which contains selected neg and pos\n",
    "      #  Y2[:, sel_samples, :] => labels and gt bboxes for num_rois bboxes which contains selected neg and pos\n",
    "      loss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])\n",
    "      losses[iter_num, 0] = loss_rpn[1]\n",
    "      losses[iter_num, 1] = loss_rpn[2]\n",
    "      \n",
    "      losses[iter_num, 2] = loss_class[1]\n",
    "      losses[iter_num, 3] = loss_class[2]\n",
    "      losses[iter_num, 4] = loss_class[3]\n",
    "      iter_num += 1\n",
    "      progbar.update(iter_num, [('rpn_cls', np.mean(losses[:iter_num, 0])), ('rpn_regr', np.mean(losses[:iter_num, 1])),\n",
    "                                      ('final_cls', np.mean(losses[:iter_num, 2])), ('final_regr', np.mean(losses[:iter_num, 3]))])\n",
    "      if iter_num == epoch_length:\n",
    "        loss_rpn_cls = np.mean(losses[:, 0])\n",
    "        loss_rpn_regr = np.mean(losses[:, 1])\n",
    "        loss_class_cls = np.mean(losses[:, 2])\n",
    "        loss_class_regr = np.mean(losses[:, 3])\n",
    "        class_acc = np.mean(losses[:, 4])\n",
    "        mean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n",
    "        rpn_accuracy_for_epoch = []\n",
    "        if config.verbose:\n",
    "          print('Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(mean_overlapping_bboxes))\n",
    "          print('Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))\n",
    "          print('Loss RPN classifier: {}'.format(loss_rpn_cls))\n",
    "          print('Loss RPN regression: {}'.format(loss_rpn_regr))\n",
    "          print('Loss Detector classifier: {}'.format(loss_class_cls))\n",
    "          print('Loss Detector regression: {}'.format(loss_class_regr))\n",
    "          print('Total loss: {}'.format(loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr))\n",
    "          print('Elapsed time: {}'.format(time.time() - start_time))\n",
    "          elapsed_time = (time.time()-start_time)/60\n",
    "        curr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n",
    "        iter_num = 0\n",
    "        start_time = time.time()\n",
    "        if curr_loss < best_loss:\n",
    "          if config.verbose:\n",
    "            print('Total loss decreased from {} to {}, saving weights'.format(best_loss,curr_loss))\n",
    "          best_loss = curr_loss\n",
    "          model_all.save_weights(config.model_path)\n",
    "        \n",
    "          new_row = {'mean_overlapping_bboxes':round(mean_overlapping_bboxes, 3), \n",
    "              'class_acc':round(class_acc, 3), \n",
    "              'loss_rpn_cls':round(loss_rpn_cls, 3), \n",
    "              'loss_rpn_regr':round(loss_rpn_regr, 3), \n",
    "              'loss_class_cls':round(loss_class_cls, 3), \n",
    "              'loss_class_regr':round(loss_class_regr, 3), \n",
    "              \n",
    "                     rr_loss':round(curr_loss, 3), \n",
    "              'elapsed_time':round(elapsed_time, 3), \n",
    "              'mAP': 0}\n",
    "          record_df = record_df.append(new_row, ignore_index=True)\n",
    "          record_df.to_csv(record_path, index=0)\n",
    "          break\n",
    "    except Exception as e:\n",
    "      print('Exception: {}'.format(e))\n",
    "      #continue\n",
    "      break\n",
    "\n",
    "print('Training complete, exiting.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AnA8LtIzNk93"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO0pbUKt8NGaV/7+BC1Z/ua",
   "collapsed_sections": [],
   "name": "Faster RCNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
